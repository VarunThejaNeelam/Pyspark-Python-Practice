{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6f22b0-b863-43cd-b472-573dea919219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.types import*\n",
    "from pyspark.sql.functions import* \n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676a2eda-6ddd-4a3d-b984-22dc91d3b46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced Stock Market Data Analysis\n",
    "Dataset:\n",
    "Historical stock prices with:\n",
    "stock_id, date, open_price, high_price, low_price, close_price, volume.\n",
    "\n",
    "Tasks:\n",
    "Compute daily price change percentage for each stock.\n",
    "Identify stocks with the highest volatility (price fluctuations) in the last 30 days.\n",
    "Find the top 5 stocks with the highest trading volume for each month.\n",
    "Compute a 7-day moving average closing price for each stock.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0788ab08-b6fb-4583-beb9-c351b58940fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"stock_id\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"open_price\", DoubleType(), False),\n",
    "    StructField(\"high_price\", DoubleType(), False),\n",
    "    StructField(\"low_price\", DoubleType(), False),\n",
    "    StructField(\"close_price\", DoubleType(), False),\n",
    "    StructField(\"volume\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"AAPL\", \"2024-03-01\", 150.0, 155.0, 149.0, 152.0, 5000000),\n",
    "    (\"AAPL\", \"2024-03-02\", 152.0, 158.0, 151.0, 157.0, 6000000),\n",
    "    (\"AAPL\", \"2024-03-03\", 157.0, 160.0, 156.0, 159.0, 5500000),\n",
    "    (\"GOOGL\", \"2024-03-01\", 2800.0, 2850.0, 2780.0, 2825.0, 3000000),\n",
    "    (\"GOOGL\", \"2024-03-02\", 2825.0, 2900.0, 2810.0, 2880.0, 3200000),\n",
    "    (\"GOOGL\", \"2024-03-03\", 2880.0, 2950.0, 2870.0, 2920.0, 3100000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(\"date\"))\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60351656-d720-4c8b-a2ca-7a3333ce4790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class stockDataAnalysis:\n",
    "    def read_data(self, path):\n",
    "        try:\n",
    "            df = spark.read.format('csv').option('header',True).load(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured during file reading: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Compute daily price change percentage for each stock.    \n",
    "    def dailyPriceChange(self, df, lag_key, partition_col, order_col):\n",
    "        # window specification\n",
    "        window_spec = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "        # Creating previous closing price column\n",
    "        updated_df = df.withColumn(\"previous_day_price\", lag(lag_key).over(window_spec))\n",
    "        # Filtered null values\n",
    "        updated_df = updated_df.filter(col(\"previous_day_price\").isNotNull())\n",
    "        # Calculating percentage \n",
    "        percentage_df = updated_df.withColumn(\n",
    "            \"percentage\", ((col(\"closing_price\") - col(\"previous_day_price\")) / col(\"previous_day_price\")) * 100)\n",
    "        return percentage_df\n",
    "    \n",
    "    # Identify stocks with the highest volatility (price fluctuations) in the last 30 days.\n",
    "    def stocksVolatility(self, df, lag_key, partition_col, order_col):\n",
    "        # window specification\n",
    "        window_spec = Window.partitionBy(partition_col).orderBy(order_col)\n",
    "        # Filtering dataframe to get last 30 days data\n",
    "        filtered_df = df.filter(col(\"date\") > date_sub(current_date(),30))\n",
    "        # Creating previous closing price column\n",
    "        updated_df = filtered_df.withColumn(\"previous_day_price\", lag(lag_key).over(window_spec))\n",
    "        # Handling nulls using fillna function\n",
    "        updated_df = updated_df.fillna(0)\n",
    "        # Calculating percentage \n",
    "        percentage_df = updated_df.withColumn(\n",
    "            \"percentage\", ((col(\"closing_price\") - col(\"previous_day_price\")) / col(\"previous_day_price\")) * 100)\n",
    "        # Aggregating standard deviation for each stock\n",
    "        aggregated_df = percentage_df.groupBy(\"stock_id\").agg(\n",
    "            stddev(\"percentage\").alias(\"volatility\")\n",
    "        )\n",
    "        # window specification for ranking\n",
    "        window_rank = Window.orderBy(desc(\"volatility\"))\n",
    "        # Ranking stocks based on this volatility\n",
    "        ranking_df = aggregated_df.withColumn(\n",
    "            \"rank\",\n",
    "            dense_rank().over(window_rank)\n",
    "        )\n",
    "        return ranking_df\n",
    "    \n",
    "    # Find the top 5 stocks with the highest trading volume for each month.\n",
    "    def highestTradingVolumes(self, df, groupBy_key, sum_key):\n",
    "        # Extracting year and month from date\n",
    "        df = df.withColumn(\"year_month\",date_format(col(\"date\"), \"yyyy-MM\"))\n",
    "        # Aggregating total volumes for each stock\n",
    "        aggregated_df = df.groupBy(groupBy_key, \"year_month\").agg(\n",
    "            sum(sum_key).alias(\"total_volumes\")\n",
    "        )\n",
    "        # window specification for ranking\n",
    "        window = Window.partitionBy(\"year_month\").orderBy(desc(\"total_volumes\"))\n",
    "        # Ranking stocks with total volumes\n",
    "        ranking_df = aggregated_df.withColumn(\"rank\", rank().over(window))\n",
    "        # filtering top 3 stocks\n",
    "        top5_stocks_df = ranking_df.filter(col(\"rank\") <= 5)\n",
    "        return top5_stocks_df\n",
    "    \n",
    "    # Compute a 7-day moving average closing price for each stock.\n",
    "    def stockMovingAvg(self, df, partition_col, order_col, avg_key):\n",
    "        # window specification for moving average\n",
    "        window_avg = Window.partitionBy(partition_col).orderBy(order_col).rowsBetween(-6, 0)\n",
    "        # Aggregating moving average for each stock\n",
    "        moving_avg_df = df.withColumn(\n",
    "            \"moving_average\",\n",
    "            avg(avg_key).over(window_avg)\n",
    "        )\n",
    "        return moving_avg_df\n",
    "    \n",
    "# Instantiating the class     \n",
    "stock_inst = stockDataAnalysis()\n",
    "\n",
    "# Reading the file from path\n",
    "stocks_df = stock_inst.read_data(\"/Volumes/rbc/rbcschema/raw/stocks\")\n",
    "\n",
    "# reading the methods \n",
    "percentage_df = stock_inst.dailyPriceChange(stocks_df, \"closing_price\", \"stock_id\", \"date\")\n",
    "\n",
    "ranking_df = stock_inst.stocksVolatility(stocks_df, \"closing_price\", \"stock_id\", \"date\")\n",
    "\n",
    "top5_stocks_df = stock_inst.highestTradingVolumes(stocks_df, \"stock_id\", \"volume\")\n",
    "\n",
    "moving_avg_df = stock_inst.stockMovingAvg(stocks_df, \"stock_id\", \"date\", \"closing_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bf5d654-2fb4-4a0d-b8b9-cbbedf3529c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
