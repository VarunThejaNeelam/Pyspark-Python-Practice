{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340bbc92-0c48-4b43-b71c-39f7375c48b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.types import*\n",
    "from pyspark.sql.functions import* \n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5deacf-7ff6-48bc-9139-6805f654333d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"customers\":\"/Volumes/rbc/rbcschema/raw/customers.csv\",\n",
    "        \"transactions\":\"/Volumes/rbc/rbcschema/raw/transactions.csv\",\n",
    "        \"join_type\":\"inner\",\n",
    "        \"groupBy_key\":\"customer_id\",\n",
    "        \"avg_key\":\"amount\",\n",
    "        \"sum_key\":\"amount\",\n",
    "        \"transformations\":[\"join\", \"avg\", \"sum\",\"rank\"],\n",
    "        \"save_format\":\"delta\",\n",
    "        \"destination\":\"/Volumes/rbc/rbcschema/curated/customer_transaction.delta\"\n",
    "    }\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customers\", StringType(), True),\n",
    "    StructField(\"transactions\", StringType(), True),\n",
    "    StructField(\"join_type\", StringType(), True),\n",
    "    StructField(\"groupBy_key\", StringType(), True),\n",
    "    StructField(\"avg_key\", StringType(), True),\n",
    "    StructField(\"sum_key\", StringType(), True),\n",
    "    StructField(\"transformations\",ArrayType(StringType()), True),\n",
    "    StructField(\"save_format\", StringType(), True),\n",
    "    StructField(\"destination\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.write.format('json').option('header',True).save(\"/Volumes/rbc/rbcschema/raw/config_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd88ccf-a484-4d39-a0c9-ea8b3f5eef37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CustomerTransactions\").getOrCreate()\n",
    "\n",
    "# Define the CustomerTransactions class\n",
    "class CustomerTransactions:\n",
    "    @staticmethod\n",
    "    def read_data(path):\n",
    "        try:\n",
    "            df = spark.read.format('csv').option('header', True).load(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while reading file {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def joining(customers, transactions, join_type):\n",
    "        return customers.join(transactions, on=\"customer_id\", how=join_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def avgTransaction(joined_df, avg_key, groupBy_key):\n",
    "        return joined_df.groupBy(groupBy_key).agg(avg(avg_key).alias(\"avg_transaction_amount\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def totalTransaction(joined_df, sum_key, groupBy_key):\n",
    "        return joined_df.groupBy(groupBy_key).agg(sum(sum_key).alias(\"total_transaction_amount\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def rankingCustomers(total_aggregated_df):\n",
    "        window_rank = Window.orderBy(desc(\"total_transaction_amount\"))\n",
    "        return total_aggregated_df.withColumn(\"rank\", rank().over(window_rank))\n",
    "\n",
    "# Instantiate the class\n",
    "cus_instance = CustomerTransactions()\n",
    "\n",
    "# Read the config JSON file\n",
    "config_df = spark.read.format('json').load(\"/Volumes/rbc/rbcschema/raw/config_file\")\n",
    "\n",
    "# Iterate through config DataFrame rows\n",
    "for row in config_df.collect():\n",
    "    customers_path = row[\"customers\"]\n",
    "    transactions_path = row[\"transactions\"]\n",
    "    join_type = row[\"join_type\"]\n",
    "    groupBy_key = row[\"groupBy_key\"]\n",
    "    avg_key = row[\"avg_key\"]\n",
    "    sum_key = row[\"sum_key\"]\n",
    "    transformations = row[\"transformations\"]  # Already an array\n",
    "\n",
    "    # Read customers and transactions data\n",
    "    customers_df = cus_instance.read_data(customers_path)\n",
    "    transactions_df = cus_instance.read_data(transactions_path)\n",
    "\n",
    "    if not customers_df or not transactions_df:\n",
    "        print(f\"Skipping processing for {customers_path} and {transactions_path} due to errors\")\n",
    "        continue\n",
    "\n",
    "    # Apply transformations step-by-step\n",
    "    result_df = None\n",
    "\n",
    "    if \"join\" in transformations:\n",
    "        result_df = cus_instance.joining(customers_df, transactions_df, join_type)\n",
    "    if \"avg\" in transformations and result_df is not None:\n",
    "        result_df = cus_instance.avgTransaction(result_df, avg_key, groupBy_key)\n",
    "    if \"sum\" in transformations and result_df is not None:\n",
    "        result_df = cus_instance.totalTransaction(result_df, sum_key, groupBy_key)\n",
    "    if \"rank\" in transformations and result_df is not None:\n",
    "        result_df = cus_instance.rankingCustomers(result_df)\n",
    "\n",
    "    # Save the final result\n",
    "    if result_df:\n",
    "        save_path = row[\"destination\"]\n",
    "        save_format = row[\"save_format\"]\n",
    "        result_df.write.format(save_format).mode(\"overwrite\").save(save_path)\n",
    "        print(f\"Saved result to {save_path} in {save_format} format\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "python_practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
