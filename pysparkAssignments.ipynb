{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b353028b-779d-4e5f-8a7c-19a20cf5771c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a2dad4-a7e9-4d76-8f37-5467f99f0859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a small pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfcec84-18d0-4f07-a716-06c590c5aff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"P1\", \"North\", \"Electronics\", 1200.50, \"2025-04-01\"),\n",
    "    (\"P2\", \"North\", \"Electronics\", 950.75, \"2025-04-01\"),\n",
    "    (\"P3\", \"North\", \"Electronics\", 850.00, \"2025-04-01\"),\n",
    "    (\"P4\", \"North\", \"Electronics\", 200.00, \"2025-04-01\"),\n",
    "    (\"P1\", \"South\", \"Furniture\", 500.00, \"2025-04-01\"),\n",
    "    (\"P5\", \"South\", \"Furniture\", 1500.00, \"2025-04-01\"),\n",
    "    (\"P6\", \"South\", \"Furniture\", 700.00, \"2025-04-01\"),\n",
    "    (\"P7\", \"East\", \"Grocery\", 300.00, \"2025-04-01\"),\n",
    "    (\"P8\", \"East\", \"Grocery\", 1200.00, \"2025-04-01\"),\n",
    "    (\"P9\", \"East\", \"Grocery\", 1000.00, \"2025-04-01\"),\n",
    "    (\"P10\", \"East\", \"Grocery\", 50.00, \"2025-04-01\"),\n",
    "]\n",
    "\n",
    "# Define schema with sale_date as string\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sale_amount\", DoubleType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(data, schema)\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "sales_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1aba64-8a49-4f3c-ac9d-d682529e239c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each region and category, find:\n",
    "Top 3 selling products by revenue\n",
    "\"\"\"\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"total_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "top3_products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d28635f-dd5c-45a5-9d07-72f5887f5f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Average revenue of top 3 products\n",
    "\"\"\"\n",
    "# Join original sales data with top 3 products (based on total revenue)\n",
    "top3_sales_df = sales_df.join(top3_products_df.select(\"region\", \"category\", \"product_id\"), \n",
    "                              on=[\"region\", \"category\", \"product_id\"], how=\"inner\")\n",
    "\n",
    "# Calculate average revenue of those top 3 products per region-category\n",
    "avg_top3_df = top3_sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    avg(\"sale_amount\").alias(\"avg_top3_revenue\")\n",
    ")\n",
    "avg_top3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca7392a-e2ac-43ce-83d6-6fde5c1fe84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Percentage contribution of each top product to its category-region revenue\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"top_product_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"top_product_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Aggregating total reveenue across each region  and category\n",
    "total_reg_cate_df = sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Joining total revenue and catgeory region level revenue\n",
    "joined_df = total_reg_cate_df.join(top3_products_df, on=[\"region\", \"category\"], how=\"inner\")\n",
    "\n",
    "# Contribution of top product in each category\n",
    "top_product_cont_df = joined_df.withColumn(\n",
    "    \"contribution\",\n",
    "    (col(\"top_product_revenue\") / col(\"total_revenue\")) * 100\n",
    ")\n",
    "top_product_cont_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e5b4d3-c72b-404e-946a-51d2b4d50b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect First and Last Purchase of Customer (Lifecycle Analysis)\n",
    "Dataset:\n",
    "orders: order_id, customer_id, order_date, order_amount\n",
    "\n",
    "🛠️ Task:\n",
    "For each customer:\n",
    "Get first and last purchase date\n",
    "Calculate days between first and last order\n",
    "Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "Also, calculate average order value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e09fb00-042e-46bb-bae9-3941fd56727f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample schema for the 'orders' DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the 'orders' DataFrame\n",
    "data = [\n",
    "    (1, 101, \"2024-01-01\", 150.0),\n",
    "    (2, 101, \"2024-03-15\", 200.0),\n",
    "    (3, 102, \"2024-01-10\", 300.0),\n",
    "    (4, 103, \"2024-01-12\", 250.0),\n",
    "    (5, 104, \"2024-02-01\", 120.0),\n",
    "    (6, 104, \"2024-04-10\", 180.0)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert the 'order_date' column to date format\n",
    "orders_df = orders_df.withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97f38b-c165-4997-8f8a-ccca44e89936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get first and last purchase date\n",
    "first_last_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    min(\"order_date\").alias(\"first_purchase_date\"),\n",
    "    max(\"order_date\").alias(\"last_purchase_date\")\n",
    ")\n",
    "# Calculate days between first and last order\n",
    "day_diff_df = first_last_df.withColumn(\n",
    "    \"day_diff\",\n",
    "    datediff(col(\"last_purchase_date\"), col(\"first_purchase_date\"))\n",
    ")\n",
    "day_diff_df.show()\n",
    "\n",
    "# Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "customer_analysis_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "customers_flags_df = customer_analysis_df.withColumn(\n",
    "    \"buyer_type\",\n",
    "    when(col(\"total_orders\") > 1, \"Repeat_Buyer\").otherwise(\"One_Time_Buyer\")\n",
    ")\n",
    "customer_flags_df.show()\n",
    "\n",
    "# Also, calculate average order value\n",
    "avg_order_val_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"order_amount\").alias(\"avg_order_value\")\n",
    ")\n",
    "window_spec = Window.orderBy(desc(\"avg_order_value\"))\n",
    "ranking_df = avg_order_val_df.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "ranking_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2089e4-a688-40ad-8f06-434c6641b4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Series Trend Analysis Per Product\n",
    "Dataset:\n",
    "daily_sales: product_id, sale_date, units_sold\n",
    "\n",
    "🛠️ Task:\n",
    "For each product:\n",
    "Calculate 7-day moving average of sales\n",
    "Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "Flag spike days (where sales > 2 * moving avg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ba098d-cc01-4a0c-b686-5f8db0e16a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"units_sold\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (101, \"2024-04-01\", 50),\n",
    "    (101, \"2024-04-02\", 55),\n",
    "    (101, \"2024-04-03\", 53),\n",
    "    (101, \"2024-04-04\", 60),\n",
    "    (101, \"2024-04-05\", 65),\n",
    "    (101, \"2024-04-06\", 70),\n",
    "    (101, \"2024-04-07\", 75),\n",
    "    (101, \"2024-04-08\", 150),  # Spike\n",
    "    (101, \"2024-04-09\", 80),\n",
    "    (102, \"2024-04-01\", 20),\n",
    "    (102, \"2024-04-02\", 22),\n",
    "    (102, \"2024-04-03\", 25),\n",
    "    (102, \"2024-04-04\", 23),\n",
    "    (102, \"2024-04-05\", 24),\n",
    "    (102, \"2024-04-06\", 21),\n",
    "    (102, \"2024-04-07\", 19),\n",
    "    (102, \"2024-04-08\", 18),\n",
    "    (102, \"2024-04-09\", 35)  # Spike\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "daily_sales_df = spark.createDataFrame(data, schema)\n",
    "daily_sales_df = daily_sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "daily_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6938152e-c772-42e3-9511-dbeb82fbd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate 7-day moving average of sales\n",
    "window = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(-6, 0)\n",
    "moving_avg_df = daily_sales_df.withColumn(\n",
    "    \"moving_avg\",\n",
    "    avg(\"units_sold\").over(window)\n",
    ")\n",
    "\n",
    "# Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "window_spec = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "product_trends_df = daily_sales_df.withColumn(\n",
    "    \"prev_day_sale\",\n",
    "    lag(\"units_sold\").over(window_spec)\n",
    ")\n",
    "sales_analysis_df = product_trends_df.withColumn(\n",
    "    \"isHigher\",\n",
    "    when(col(\"units_sold\") > col(\"prev_day_sale\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Identifying consequtive streaks\n",
    "windows = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "random_rows_df = sales_analysis_df.withColumn(\n",
    "    \"rn\", \n",
    "    row_number().over(windows)\n",
    ").filter(\"prev_day_sale is not null\")\n",
    "\n",
    "# Creating groups to find consequtives by substracting date and rownumber\n",
    "conse_grop_df = random_rows_df.withColumn(\n",
    "    \"grouping\",\n",
    "    date_sub(\"sale_date\", \"rn\")\n",
    ")\n",
    "aggregated_df = conse_grop_df.groupBy(\"product_id\", \"grouping\").agg(\n",
    "    sum(when(col(\"isHigher\") == 'Yes'),1).alias(\"total_growths\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for consequtives for each product\n",
    "window_rank = Window.partitionBy(\"product\").orderBy(desc(\"total_growths\"))\n",
    "product_cons_rank_df = aggregated_df.withColumn(\"rank\", dense_rank().over(window_rank))\n",
    "product_cons_rank_df.show()\n",
    "\n",
    "# Flag spike days (where sales > 2 * moving avg)\n",
    "\n",
    "# Getting only last row per product to get moving average to join with original dataframe\n",
    "window_last = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "product_moving_avg_df = moving_avg_df.withColumn(\"last_mov_avg\", last(\"moving_avg\").over(window_last))\n",
    "product_moving_avg_df = product_moving_avg_df.distinct()\n",
    "\n",
    "# Joining with original dataframe\n",
    "joined_df = daily_sales_df.join(product_moving_avg_df.select(\"product\", \"last_mov_avg\"), on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Filetering sales where greater than moving average\n",
    "filtered_df = joined_df.filter(col(\"units_sold\") > 2 * col(\"last_mov_avg\"))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba11c22-02f9-4d24-96d0-8c89a56db2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"user_id\": \"u1\",\n",
    "        \"events\": [\n",
    "            {\"event_type\": \"click\", \"timestamp\": \"2024-01-01T10:00:00\"},\n",
    "            {\"event_type\": \"purchase\", \"timestamp\": \"2024-01-01T10:05:00\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"events\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"event_type\", StringType(), True),\n",
    "                        StructField(\"timestamp\", StringType(), True),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "json_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Exploding the json dataframe with explode function\n",
    "exploded_df = json_df.withColumn(\"event\", explode(\"events\"))\n",
    "\n",
    "final_df = exploded_df.select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"event.event_type\").alias(\"event_type\"),\n",
    "    col(\"event.timestamp\").alias(\"timestamp\"),\n",
    ")\n",
    "\n",
    "final_df = final_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# For each event, calculate time since previous event (per user)\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "time_diff_df = final_df.withColumn(\n",
    "    \"previous_event_time\", lag(\"timestamp\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"time_diff_seconds\",\n",
    "    unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"previous_event_time\")),\n",
    ").withColumn(\n",
    "    \"time_diff_readable\",\n",
    "    from_unixtime(col(\"time_diff_seconds\"), \"HH:mm:ss\")\n",
    ")\n",
    "time_diff_df.filter(\"previous_event_time IS Not Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd0fe6a-042f-48d6-ac2c-db563bd3c2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complex Pivot with Aggregation and Ratio Calculation\n",
    "Dataset:\n",
    "transactions: user_id, category, amount, transaction_date\n",
    "\n",
    "🛠️ Task:\n",
    "Pivot to get total amount spent per category as separate columns\n",
    "Add total amount and ratio of each category to total\n",
    "\"\"\"\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"u1\", \"grocery\", 120.0, \"2024-01-01\"),\n",
    "    (\"u1\", \"electronics\", 500.0, \"2024-01-02\"),\n",
    "    (\"u1\", \"grocery\", 80.0, \"2024-01-03\"),\n",
    "    (\"u2\", \"grocery\", 200.0, \"2024-01-01\"),\n",
    "    (\"u2\", \"fashion\", 300.0, \"2024-01-02\"),\n",
    "    (\"u3\", \"fashion\", 150.0, \"2024-01-03\"),\n",
    "    (\"u3\", \"electronics\", 700.0, \"2024-01-04\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)  # You can also make it DateType if needed\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "transactions_df = spark.createDataFrame(data, schema)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", to_date(\"transaction_date\"))\n",
    "\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ee59d4-fa17-480a-bbd8-201801e1701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivoted the dataframe to find each category amount\n",
    "pivoted_df = transactions_df.groupBy(\"user_id\").pivot(\"category\").sum(\"amount\")\n",
    "\n",
    "# Aggregating total amount per user\n",
    "aggregated_df = transactions_df.groupBy(\"user_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "# Joining both pivoted and aggregated dataframes\n",
    "joined_df = pivoted_df.join(aggregated_df, on=\"user_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "# Computing ratios for each category\n",
    "ratio_df = (joined_df\n",
    "            .withColumn(\"electronics_ratio\", round(coalesce(col(\"electronics\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"fashion_ratio\", round(coalesce(col(\"fashion\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"grocery_ratio\", round(coalesce(col(\"grocery\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            )\n",
    "\n",
    "ratio_df.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e9a426-5619-421a-bc17-0e0914075215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic way to calculate category ratios \n",
    "from pyspark.sql.functions import col, round, coalesce, lit\n",
    "\n",
    "# Existing columns\n",
    "basic_columns = joined_df.columns\n",
    "\n",
    "# List of category columns (excluding user_id and total_amount)\n",
    "category_columns = [c for c in basic_columns if c not in (\"user_id\", \"total_amount\")]\n",
    "\n",
    "# Start with existing columns\n",
    "final_cols = [col(\"user_id\")] + [col(c) for c in category_columns] + [col(\"total_amount\")]\n",
    "\n",
    "# Add ratio columns dynamically\n",
    "for cat in category_columns:\n",
    "    ratio_col_name = f\"{cat}_ratio\"\n",
    "    final_cols.append(\n",
    "        round(coalesce(col(cat), lit(0)) / col(\"total_amount\"), 2).alias(ratio_col_name)\n",
    "    )\n",
    "\n",
    "# Select all\n",
    "dynamic_ratio_df = joined_df.select(*final_cols)\n",
    "\n",
    "dynamic_ratio_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9b9793-e2cd-4714-ac39-79f25b9d7547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Financial Transactions – Fraud Pattern Detection\n",
    "Dataset:\n",
    "transactions: transaction_id, account_id, amount, timestamp, location\n",
    "\n",
    "🛠️ Tasks:\n",
    "For each account:\n",
    "Find if more than 3 transactions occur within 1 minute → mark as \"suspicious burst\"\n",
    "Flag any transaction where location changes between two transactions < 5 minutes apart\n",
    "Output flagged transactions with reason (burst, suspicious_location)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da17a2a-b595-46b9-9527-86d0c8b332f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 101, 500, \"2025-04-29 10:00:00\", \"New York\"),\n",
    "    (2, 101, 300, \"2025-04-29 10:01:00\", \"New York\"),\n",
    "    (3, 101, 100, \"2025-04-29 10:02:00\", \"New York\"),\n",
    "    (4, 101, 200, \"2025-04-29 10:03:00\", \"New York\"),\n",
    "    (5, 101, 150, \"2025-04-29 10:05:00\", \"Los Angeles\"),\n",
    "    (6, 102, 700, \"2025-04-29 09:50:00\", \"Chicago\"),\n",
    "    (7, 102, 300, \"2025-04-29 09:51:00\", \"Chicago\"),\n",
    "    (8, 102, 200, \"2025-04-29 09:52:00\", \"Chicago\"),\n",
    "    (9, 102, 150, \"2025-04-29 09:53:00\", \"Chicago\"),\n",
    "    (10, 102, 100, \"2025-04-29 09:54:00\", \"Chicago\"),\n",
    "    (11, 103, 1000, \"2025-04-29 10:30:00\", \"San Francisco\"),\n",
    "    (12, 103, 200, \"2025-04-29 10:35:00\", \"San Francisco\"),\n",
    "    (13, 103, 300, \"2025-04-29 10:36:00\", \"San Francisco\"),\n",
    "    (14, 103, 100, \"2025-04-29 10:40:00\", \"Los Angeles\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a49dc75-3950-4c18-aa63-ae7912678a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find if more than 3 transactions occur within 1 minute → mark as \"suspicious burst\"\n",
    "\n",
    "# Converting timestamp to seconds using unix_timestamp function\n",
    "df = df.withColumn(\"time_in_seconds\", unix_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# window specification for rolling 1 minute window\n",
    "window_spec = Window.partitionBy(\"account_id\").orderBy(\"timestamp\").rangeBetween(-60, 0)\n",
    "\n",
    "# Counting total transactions per account in rolling 1 window\n",
    "aggregated_df = df.withColumn(\n",
    "    \"total_transactions\", \n",
    "    count(\"transaction_id\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Filtering transactions less than or equal to 3 to flag them as suspecious\n",
    "suspecious_df = aggregated_df.filter(col(\"total_transactions\") > 3)\n",
    "suspecious_df = suspecious_df.withColumn(\"transaction_flag\", lit(\"suspecious_burst\"))\n",
    "\n",
    "suspecious_burst_df = suspecious_df.select(\n",
    "    col(\"transaction_id\"),\n",
    "    col(\"account_id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"location\"),\n",
    "    col(\"transaction_flag\")\n",
    ")\n",
    "\n",
    "# Flag any transaction where location changes between two transactions < 5 minutes apart\n",
    "\n",
    "# window specification to find previous transaction location and timestamp\n",
    "window = Window.partitionBy(\"account_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# Creating prev location and prev timestamp columns\n",
    "transaction_details_df = df.withColumn(\n",
    "    \"prev_location\",\n",
    "    lag(\"location\").over(window)\n",
    ").withColumn(\n",
    "    \"prev_timestamp\",\n",
    "    lag(\"timestamp\").over(window)\n",
    ")\n",
    "\n",
    "# Identifying differences between timestamp and prevtimestamp \n",
    "transaction_analysis_df = transaction_details_df.withColumn(\n",
    "    \"time_diff\",\n",
    "    (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"prev_timestamp\")))\n",
    ")\n",
    "\n",
    "# Filtering suspecious locations transactions\n",
    "suspecious_loc_df = transaction_analysis_df.filter(\n",
    "    (col(\"time_diff\") < 300) & \n",
    "    (col(\"location\") != col(\"prev_location\"))\n",
    ")\n",
    "\n",
    "suspecious_loc_df = suspecious_loc_df.withColumn(\"transaction_flag\", lit(\"suspecious_location\"))\n",
    "\n",
    "suspecious_loc_df = suspecious_loc_df.select(\n",
    "    col(\"transaction_id\"),\n",
    "    col(\"account_id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"location\"),\n",
    "    col(\"transaction_flag\")\n",
    ")\n",
    "\n",
    "suspecious_trans_df = suspecious_burst_df.union(suspecious_loc_df)\n",
    "suspecious_trans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc40c453-b027-4097-956e-9149e169ede7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cart Abandonment Tracking\n",
    "Dataset:\n",
    "cart_events: user_id, event_type (add_to_cart, purchase), timestamp\n",
    "\n",
    "🛠️ Tasks:\n",
    "For each cart session (based on 1-hour inactivity):\n",
    "Count number of items added\n",
    "Whether the cart was purchased\n",
    "Calculate cart abandonment rate\n",
    "Final output per user per session\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7cfdb7-347b-407c-a338-f7459de31a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),  # 'add_to_cart' or 'purchase'\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"user_1\", \"add_to_cart\", \"2025-04-29 10:00:00\"),\n",
    "    (\"user_1\", \"add_to_cart\", \"2025-04-29 10:10:00\"),\n",
    "    (\"user_1\", \"purchase\",     \"2025-04-29 10:20:00\"),\n",
    "    (\"user_1\", \"add_to_cart\", \"2025-04-29 12:00:00\"),\n",
    "    (\"user_2\", \"add_to_cart\", \"2025-04-29 11:00:00\"),\n",
    "    (\"user_2\", \"add_to_cart\", \"2025-04-29 11:45:00\"),\n",
    "    (\"user_2\", \"add_to_cart\", \"2025-04-29 13:00:00\"),\n",
    "    (\"user_2\", \"purchase\",     \"2025-04-29 13:30:00\"),\n",
    "    (\"user_3\", \"add_to_cart\", \"2025-04-29 14:00:00\"),\n",
    "]\n",
    "\n",
    "# Convert timestamps and create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Show schema and data\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bd6198-dc53-45db-93c0-6f8af9f54a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CartAbandonmentTracking:\n",
    "    def read_file(self, path):\n",
    "        try:\n",
    "            df = spark.read.format('csv').option('header', True).load(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured while file reading: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def user_analysis(self, df):\n",
    "        # Ensure timestamp is in correct format\n",
    "        df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "        # Create lag column to get previous event timestamp\n",
    "        window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "        df = df.withColumn(\n",
    "            \"previous_timestamp\",\n",
    "            lag(\"timestamp\").over(window_spec)\n",
    "        )  \n",
    "\n",
    "        # Checking time difference in minutes to create sessions\n",
    "        df = df.withColumn(\n",
    "            \"time_diff_minutes\",\n",
    "            (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"previous_timestamp\"))) / 60\n",
    "        )\n",
    "\n",
    "        # Flag when a new session starts (inactivity > 60 min)\n",
    "        df = df.withColumn(\n",
    "            \"session_flag\",\n",
    "            when(col(\"time_diff_minutes\").isNull() | (col(\"time_diff_minutes\") > 60), 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        # Generate incremental session IDs using cumulative sum\n",
    "        df = df.withColumn(\n",
    "            \"session_number\",\n",
    "            sum(\"session_flag\").over(window_spec)\n",
    "        )\n",
    "\n",
    "        # Aggregating total addtocart and purchase count\n",
    "        agg_df = df.groupBy(\"user_id\", \"sessionID\").agg(\n",
    "            count(when(col(\"event_type\") == \"add_to_cart\", True)).alias(\"total_events_add\"),\n",
    "            count(when(col(\"event_type\") == \"purchase\", True)).alias(\"total_purchases\")\n",
    "        )\n",
    "\n",
    "        # Count user sessions and abondoned sessions\n",
    "        user_session_stats_df = agg_df.groupBy(\"user_id\").agg(\n",
    "            count(\"sessionID\").alias(\"total_sessions\"),\n",
    "            count(when(col(\"total_purchases\") == 0, True)).alias(\"abondoned_sessions\") \n",
    "        )\n",
    "\n",
    "        # Abondonment rate per user\n",
    "        result_df = user_session_stats_df.withColumn(\n",
    "            \"abondonment_rate\",\n",
    "            (col(\"abondonted_sessions\") / col(\"total_sessions\")) * 100 \n",
    "        )\n",
    "        return result_df\n",
    "    \n",
    "cart_trac_inst = CartAbandonmentTracking\n",
    "\n",
    "# Reading file from path\n",
    "df = cart_trac_inst.read_file(\"dbfs://FileStore/raw/carts_tracking\")\n",
    "\n",
    "result = cart_trac_inst.user_analysis(df)\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cba2c64-433d-47e4-8593-afa352addc9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shift Scheduling Conflicts\n",
    "Dataset:\n",
    "employee_shifts: emp_id, shift_start, shift_end\n",
    "\n",
    "🛠️ Tasks:\n",
    "Identify employees with overlapping shifts\n",
    "Report all conflict pairs: emp_id, conflict_start, conflict_end\n",
    "Use self-join on emp_id, and time overlap logic:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19521899-c4b2-445f-8342-eccccf1781e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema with string timestamps\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"shift_start\", StringType(), True),\n",
    "    StructField(\"shift_end\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Sample data (timestamps as strings)\n",
    "data = [\n",
    "    (101, \"2023-05-01 09:00:00\", \"2023-05-01 13:00:00\"),\n",
    "    (101, \"2023-05-01 12:00:00\", \"2023-05-01 16:00:00\"),  # Overlap\n",
    "    (101, \"2023-05-01 17:00:00\", \"2023-05-01 20:00:00\"),  # No overlap\n",
    "    (102, \"2023-05-02 08:00:00\", \"2023-05-02 12:00:00\"),\n",
    "    (102, \"2023-05-02 11:00:00\", \"2023-05-02 15:00:00\"),  # Overlap\n",
    "    (103, \"2023-05-03 10:00:00\", \"2023-05-03 14:00:00\"),  # No conflict\n",
    "]\n",
    "\n",
    "# Create DataFrame with string timestamps\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert to actual timestamps\n",
    "df = df.withColumn(\"shift_start\", to_timestamp(col(\"shift_start\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"shift_end\", to_timestamp(col(\"shift_end\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Show the DataFrame and schema\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb1ec68-2e23-4f19-b9cc-44c27271a4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Applying self join to identify overlap shifts for each employee\n",
    "overlap_shifts_df = df.alias(\"df1\").join(\n",
    "    df.alias(\"df2\"),\n",
    "    (col(\"df1.emp_id\") == col(\"df2.emp_id\")) &\n",
    "    (col(\"df1.shift_start\") < col(\"df2.shift_start\")) &  # avoids symmetric pairs and self-pairs\n",
    "    (col(\"df1.shift_end\") > col(\"df2.shift_start\")) &\n",
    "    (col(\"df1.shift_start\") < col(\"df2.shift_end\"))\n",
    ")\n",
    "\n",
    "shifts_conflicts_df = overlap_shifts_df.select(\n",
    "    col(\"df1.emp_id\").alias(\"emp_id\"),\n",
    "    col(\"df1.shift_start\").alias(\"shift1_start\"),\n",
    "    col(\"df1.shift_end\").alias(\"shift1_end\"),\n",
    "    col(\"df2.shift_start\").alias(\"conflict_start\"),\n",
    "    col(\"df2.shift_end\").alias(\"conflict_end\")\n",
    ")\n",
    "\n",
    "shifts_conflicts_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3407ff53-08ad-4cc8-aebf-07289d26f0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Product Lifecycle Movement\n",
    "Dataset:\n",
    "product_events: product_id, event_type, event_date\n",
    "(event types: launched, promoted, discounted, discontinued)\n",
    "\n",
    "🛠️ Tasks:\n",
    "For each product:\n",
    "Track how long it stays in each state\n",
    "Output the full lifecycle with duration\n",
    "Flag products that never reached discontinued state\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df516ff5-bb8b-4faf-99c9-181d9a502c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),  # Keep as string for now\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"P1\", \"launched\", \"2023-01-01\"),\n",
    "    (\"P1\", \"promoted\", \"2023-02-01\"),\n",
    "    (\"P1\", \"discounted\", \"2023-03-01\"),\n",
    "    (\"P1\", \"discontinued\", \"2023-04-01\"),\n",
    "\n",
    "    (\"P2\", \"launched\", \"2023-01-10\"),\n",
    "    (\"P2\", \"promoted\", \"2023-02-15\"),\n",
    "    (\"P2\", \"discounted\", \"2023-03-15\"),\n",
    "    # No discontinued for P2\n",
    "\n",
    "    (\"P3\", \"launched\", \"2023-05-01\"),\n",
    "    (\"P3\", \"discontinued\", \"2023-06-01\"),\n",
    "    # Skipped other events\n",
    "\n",
    "    (\"P4\", \"launched\", \"2023-07-01\"),\n",
    "    # Only launched\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert event_date to date type\n",
    "df = df.withColumn(\"event_date\", to_date(col(\"event_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e42f6a-01ba-48eb-8f7d-931096624ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Product lifecycle\n",
    "# window specification for lag function\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"event_date\")\n",
    "prev_events_df = df.withColumn(\n",
    "    \"prev_event_date\",\n",
    "    lag(\"event_date\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Checking duration between events for each product\n",
    "event_durations_df = prev_events_df.withColumn(\n",
    "    \"duration\",\n",
    "    date_diff(\"event_date\", \"prev_event_date\")\n",
    ")\n",
    "event_durations_df.show()\n",
    "\n",
    "# Filtering only discontinued event \n",
    "filtered_df = event_durations_df.filter(col(\"event_type\") == \"discontinued\")\n",
    "filtered_df = filtered_df.select(\n",
    "    col(\"product_id\"),\n",
    "    col(\"event_type\").alias(\"discontinued_event\")\n",
    ")\n",
    "\n",
    "# Joining products which have discontinued event with oiginal dataframe\n",
    "joined_df = df.join(filtered_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Flagging products which doesn't have discontinued event\n",
    "flagged_df = joined_df.withColumn(\n",
    "    \"is_discontinued\",\n",
    "    when(col(\"discontinued_event\").isNull(), 'No').otherwise('Yes')\n",
    ")\n",
    "final_df = flagged_df.select(\"product_id\", \"discontinued_event\", \"is_discontinued\").dropDuplicates([\"product_id\"])\n",
    "\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d51ee57b-58bc-4316-93e4-904524787784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build User Graph (Advanced Joins)\n",
    "Dataset:\n",
    "\n",
    "messages: sender_id, receiver_id, message_time\n",
    "\n",
    "🛠️ Tasks:\n",
    "For each user, find:\n",
    "Total number of distinct connections (sent or received)\n",
    "Top 3 most messaged people\n",
    "Average message frequency (messages/day)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48fcd856-9fb6-4a11-975d-2c754e219baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "#spark = SparkSession.builder.appName(\"UserGraph\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"sender_id\", StringType(), True),\n",
    "    StructField(\"receiver_id\", StringType(), True),\n",
    "    StructField(\"message_time\", StringType(), True)  # Keep as string initially\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"U1\", \"U2\", \"2023-01-01 10:00:00\"),\n",
    "    (\"U2\", \"U1\", \"2023-01-01 11:00:00\"),\n",
    "    (\"U1\", \"U3\", \"2023-01-02 09:30:00\"),\n",
    "    (\"U3\", \"U1\", \"2023-01-02 10:00:00\"),\n",
    "    (\"U1\", \"U2\", \"2023-01-03 08:00:00\"),\n",
    "    (\"U2\", \"U4\", \"2023-01-03 09:00:00\"),\n",
    "    (\"U4\", \"U2\", \"2023-01-03 10:00:00\"),\n",
    "    (\"U1\", \"U5\", \"2023-01-04 12:00:00\"),\n",
    "    (\"U1\", \"U2\", \"2023-01-05 14:00:00\"),\n",
    "    (\"U2\", \"U1\", \"2023-01-06 16:00:00\"),\n",
    "    (\"U5\", \"U1\", \"2023-01-07 17:00:00\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"message_time\", to_timestamp(\"message_time\"))\n",
    "\n",
    "# Show data\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841bcdb8-45de-42fa-83c9-1e92df7cf3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # For each user Total number of distinct connections (sent or received)\n",
    "\n",
    " # Joining the same dataset by self join with senderid\n",
    "joined_df = df.alias(\"user1\").join(\n",
    "     df.alias(\"user2\"),\n",
    "     col(\"user1.sender_id\") == col(\"user2.sender_id\"),\n",
    "     \"inner\"\n",
    ")\n",
    "sender_con_df = joined_df.select(\n",
    "    col(\"user1.sender_id\").alias(\"user_id\"),\n",
    "    col(\"user2.receiver_id\").alias(\"connection_id\")\n",
    ")\n",
    "\n",
    " # Joining the same dataset by self join with receiverid\n",
    "joined_df = df.alias(\"user1\").join(\n",
    "     df.alias(\"user2\"),\n",
    "     col(\"user1.receiver_id\") == col(\"user2.receiver_id\"),\n",
    "     \"inner\"\n",
    ")\n",
    "receiver_con_df = joined_df.select(\n",
    "    col(\"user1.receiver_id\").alias(\"user_id\"),\n",
    "    col(\"user2.sender_id\").alias(\"connection_id\")\n",
    ")\n",
    "\n",
    "merge_df = sender_con_df.union(receiver_con_df)\n",
    "# Aggregating total connection for users\n",
    "agg_df = merge_df.groupBy(\"user_id\").agg(\n",
    "    countDistinct(\"connection_id\").alias(\"total_connections\") \n",
    ")\n",
    "agg_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238b3199-c465-4ada-b190-77346aa2d955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 3 most messaged people per user\n",
    "total_messages_df = df.groupBy(\"receiver_id\", \"sender_id\").agg(\n",
    "    count(\"*\").alias(\"total_messages\")\n",
    ")\n",
    "\n",
    "# Ranking senders for each receiver \n",
    "window_spec = Window.partitionBy(\"receiver_id\").orderBy(desc(\"total_messages\"))\n",
    "ranking_df = total_messages_df.withColumn(\n",
    "    \"rnk\",\n",
    "    dense_rank().over(window_spec)\n",
    ") \n",
    "\n",
    "filtered_df = ranking_df.filter(col(\"rnk\") <= 3)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6a9e96-e0b6-4594-90a3-02f83ed9afcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Average message frequency (messages/day)\n",
    "df.show()\n",
    "\n",
    "# Aggregating total messages and active days\n",
    "total_mes_df = df.groupBy(\"sender_id\").agg(\n",
    "    count(\"*\").alias(\"total_messages\"),\n",
    "    countDistinct(\"message_time\").alias(\"active_days\")\n",
    ")\n",
    "\n",
    "# Message frequency for each user\n",
    "msg_fre_df = total_mes_df.withColumn(\n",
    "    \"frequency\",\n",
    "    (col(\"total_messages\") / col(\"active_days\"))\n",
    ")\n",
    "msg_fre_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfef85a6-91c1-4fad-aa43-9e1af547d728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Rollup Aggregation (Multilevel Grouping)\n",
    "Dataset:\n",
    "sales: region, country, state, product, revenue\n",
    "\n",
    "🛠️ Tasks:\n",
    "Output:\n",
    "Total revenue at each level: state, country, region\n",
    "Subtotals (e.g., country = All, region = All)\n",
    "Include a grand total\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a452e5c2-ef12-4f4d-8102-b936265d1317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"revenue\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"North America\", \"USA\", \"California\", \"Product A\", 1000.0),\n",
    "    (\"North America\", \"USA\", \"Texas\", \"Product B\", 1500.0),\n",
    "    (\"North America\", \"Canada\", \"Ontario\", \"Product C\", 800.0),\n",
    "    (\"Europe\", \"Germany\", \"Bavaria\", \"Product D\", 1200.0),\n",
    "    (\"Europe\", \"Germany\", \"Berlin\", \"Product E\", 900.0),\n",
    "    (\"Europe\", \"France\", \"Paris\", \"Product F\", 1100.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the data\n",
    "df.show()\n",
    "\n",
    "\n",
    "# 🧩 Apply rollup on region, country, state\n",
    "result = (\n",
    "    df.rollup(\"region\", \"country\", \"state\")\n",
    "      .agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "# 🧹 Replace nulls with 'All' to show subtotals clearly\n",
    "final_result = (\n",
    "    result.select(\n",
    "        coalesce(\"region\", lit(\"All\")).alias(\"region\"),\n",
    "        coalesce(\"country\", lit(\"All\")).alias(\"country\"),\n",
    "        coalesce(\"state\", lit(\"All\")).alias(\"state\"),\n",
    "        \"total_revenue\"\n",
    "    )\n",
    "    .orderBy(\"region\", \"country\", \"state\")\n",
    ")\n",
    "\n",
    "final_result.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pysparkAssignments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
