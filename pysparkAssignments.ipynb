{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b353028b-779d-4e5f-8a7c-19a20cf5771c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a2dad4-a7e9-4d76-8f37-5467f99f0859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a small pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfcec84-18d0-4f07-a716-06c590c5aff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"P1\", \"North\", \"Electronics\", 1200.50, \"2025-04-01\"),\n",
    "    (\"P2\", \"North\", \"Electronics\", 950.75, \"2025-04-01\"),\n",
    "    (\"P3\", \"North\", \"Electronics\", 850.00, \"2025-04-01\"),\n",
    "    (\"P4\", \"North\", \"Electronics\", 200.00, \"2025-04-01\"),\n",
    "    (\"P1\", \"South\", \"Furniture\", 500.00, \"2025-04-01\"),\n",
    "    (\"P5\", \"South\", \"Furniture\", 1500.00, \"2025-04-01\"),\n",
    "    (\"P6\", \"South\", \"Furniture\", 700.00, \"2025-04-01\"),\n",
    "    (\"P7\", \"East\", \"Grocery\", 300.00, \"2025-04-01\"),\n",
    "    (\"P8\", \"East\", \"Grocery\", 1200.00, \"2025-04-01\"),\n",
    "    (\"P9\", \"East\", \"Grocery\", 1000.00, \"2025-04-01\"),\n",
    "    (\"P10\", \"East\", \"Grocery\", 50.00, \"2025-04-01\"),\n",
    "]\n",
    "\n",
    "# Define schema with sale_date as string\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sale_amount\", DoubleType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(data, schema)\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "sales_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1aba64-8a49-4f3c-ac9d-d682529e239c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each region and category, find:\n",
    "Top 3 selling products by revenue\n",
    "\"\"\"\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"total_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "top3_products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d28635f-dd5c-45a5-9d07-72f5887f5f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Average revenue of top 3 products\n",
    "\"\"\"\n",
    "# Join original sales data with top 3 products (based on total revenue)\n",
    "top3_sales_df = sales_df.join(top3_products_df.select(\"region\", \"category\", \"product_id\"), \n",
    "                              on=[\"region\", \"category\", \"product_id\"], how=\"inner\")\n",
    "\n",
    "# Calculate average revenue of those top 3 products per region-category\n",
    "avg_top3_df = top3_sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    avg(\"sale_amount\").alias(\"avg_top3_revenue\")\n",
    ")\n",
    "avg_top3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca7392a-e2ac-43ce-83d6-6fde5c1fe84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Percentage contribution of each top product to its category-region revenue\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"top_product_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"top_product_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Aggregating total reveenue across each region  and category\n",
    "total_reg_cate_df = sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Joining total revenue and catgeory region level revenue\n",
    "joined_df = total_reg_cate_df.join(top3_products_df, on=[\"region\", \"category\"], how=\"inner\")\n",
    "\n",
    "# Contribution of top product in each category\n",
    "top_product_cont_df = joined_df.withColumn(\n",
    "    \"contribution\",\n",
    "    (col(\"top_product_revenue\") / col(\"total_revenue\")) * 100\n",
    ")\n",
    "top_product_cont_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e5b4d3-c72b-404e-946a-51d2b4d50b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect First and Last Purchase of Customer (Lifecycle Analysis)\n",
    "Dataset:\n",
    "orders: order_id, customer_id, order_date, order_amount\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "For each customer:\n",
    "Get first and last purchase date\n",
    "Calculate days between first and last order\n",
    "Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "Also, calculate average order value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e09fb00-042e-46bb-bae9-3941fd56727f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample schema for the 'orders' DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the 'orders' DataFrame\n",
    "data = [\n",
    "    (1, 101, \"2024-01-01\", 150.0),\n",
    "    (2, 101, \"2024-03-15\", 200.0),\n",
    "    (3, 102, \"2024-01-10\", 300.0),\n",
    "    (4, 103, \"2024-01-12\", 250.0),\n",
    "    (5, 104, \"2024-02-01\", 120.0),\n",
    "    (6, 104, \"2024-04-10\", 180.0)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert the 'order_date' column to date format\n",
    "orders_df = orders_df.withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97f38b-c165-4997-8f8a-ccca44e89936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get first and last purchase date\n",
    "first_last_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    min(\"order_date\").alias(\"first_purchase_date\"),\n",
    "    max(\"order_date\").alias(\"last_purchase_date\")\n",
    ")\n",
    "# Calculate days between first and last order\n",
    "day_diff_df = first_last_df.withColumn(\n",
    "    \"day_diff\",\n",
    "    datediff(col(\"last_purchase_date\"), col(\"first_purchase_date\"))\n",
    ")\n",
    "day_diff_df.show()\n",
    "\n",
    "# Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "customer_analysis_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "customers_flags_df = customer_analysis_df.withColumn(\n",
    "    \"buyer_type\",\n",
    "    when(col(\"total_orders\") > 1, \"Repeat_Buyer\").otherwise(\"One_Time_Buyer\")\n",
    ")\n",
    "customer_flags_df.show()\n",
    "\n",
    "# Also, calculate average order value\n",
    "avg_order_val_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"order_amount\").alias(\"avg_order_value\")\n",
    ")\n",
    "window_spec = Window.orderBy(desc(\"avg_order_value\"))\n",
    "ranking_df = avg_order_val_df.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "ranking_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2089e4-a688-40ad-8f06-434c6641b4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Series Trend Analysis Per Product\n",
    "Dataset:\n",
    "daily_sales: product_id, sale_date, units_sold\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "For each product:\n",
    "Calculate 7-day moving average of sales\n",
    "Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "Flag spike days (where sales > 2 * moving avg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ba098d-cc01-4a0c-b686-5f8db0e16a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"units_sold\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (101, \"2024-04-01\", 50),\n",
    "    (101, \"2024-04-02\", 55),\n",
    "    (101, \"2024-04-03\", 53),\n",
    "    (101, \"2024-04-04\", 60),\n",
    "    (101, \"2024-04-05\", 65),\n",
    "    (101, \"2024-04-06\", 70),\n",
    "    (101, \"2024-04-07\", 75),\n",
    "    (101, \"2024-04-08\", 150),  # Spike\n",
    "    (101, \"2024-04-09\", 80),\n",
    "    (102, \"2024-04-01\", 20),\n",
    "    (102, \"2024-04-02\", 22),\n",
    "    (102, \"2024-04-03\", 25),\n",
    "    (102, \"2024-04-04\", 23),\n",
    "    (102, \"2024-04-05\", 24),\n",
    "    (102, \"2024-04-06\", 21),\n",
    "    (102, \"2024-04-07\", 19),\n",
    "    (102, \"2024-04-08\", 18),\n",
    "    (102, \"2024-04-09\", 35)  # Spike\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "daily_sales_df = spark.createDataFrame(data, schema)\n",
    "daily_sales_df = daily_sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "daily_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6938152e-c772-42e3-9511-dbeb82fbd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate 7-day moving average of sales\n",
    "window = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(-6, 0)\n",
    "moving_avg_df = daily_sales_df.withColumn(\n",
    "    \"moving_avg\",\n",
    "    avg(\"units_sold\").over(window)\n",
    ")\n",
    "\n",
    "# Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "window_spec = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "product_trends_df = daily_sales_df.withColumn(\n",
    "    \"prev_day_sale\",\n",
    "    lag(\"units_sold\").over(window_spec)\n",
    ")\n",
    "sales_analysis_df = product_trends_df.withColumn(\n",
    "    \"isHigher\",\n",
    "    when(col(\"units_sold\") > col(\"prev_day_sale\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Identifying consequtive streaks\n",
    "windows = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "random_rows_df = sales_analysis_df.withColumn(\n",
    "    \"rn\", \n",
    "    row_number().over(windows)\n",
    ").filter(\"prev_day_sale is not null\")\n",
    "\n",
    "# Creating groups to find consequtives by substracting date and rownumber\n",
    "conse_grop_df = random_rows_df.withColumn(\n",
    "    \"grouping\",\n",
    "    date_sub(\"sale_date\", \"rn\")\n",
    ")\n",
    "aggregated_df = conse_grop_df.groupBy(\"product_id\", \"grouping\").agg(\n",
    "    sum(when(col(\"isHigher\") == 'Yes'),1).alias(\"total_growths\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for consequtives for each product\n",
    "window_rank = Window.partitionBy(\"product\").orderBy(desc(\"total_growths\"))\n",
    "product_cons_rank_df = aggregated_df.withColumn(\"rank\", dense_rank().over(window_rank))\n",
    "product_cons_rank_df.show()\n",
    "\n",
    "# Flag spike days (where sales > 2 * moving avg)\n",
    "\n",
    "# Getting only last row per product to get moving average to join with original dataframe\n",
    "window_last = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "product_moving_avg_df = moving_avg_df.withColumn(\"last_mov_avg\", last(\"moving_avg\").over(window_last))\n",
    "product_moving_avg_df = product_moving_avg_df.distinct()\n",
    "\n",
    "# Joining with original dataframe\n",
    "joined_df = daily_sales_df.join(product_moving_avg_df.select(\"product\", \"last_mov_avg\"), on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Filetering sales where greater than moving average\n",
    "filtered_df = joined_df.filter(col(\"units_sold\") > 2 * col(\"last_mov_avg\"))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba11c22-02f9-4d24-96d0-8c89a56db2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"user_id\": \"u1\",\n",
    "        \"events\": [\n",
    "            {\"event_type\": \"click\", \"timestamp\": \"2024-01-01T10:00:00\"},\n",
    "            {\"event_type\": \"purchase\", \"timestamp\": \"2024-01-01T10:05:00\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"events\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"event_type\", StringType(), True),\n",
    "                        StructField(\"timestamp\", StringType(), True),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "json_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Exploding the json dataframe with explode function\n",
    "exploded_df = json_df.withColumn(\"event\", explode(\"events\"))\n",
    "\n",
    "final_df = exploded_df.select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"event.event_type\").alias(\"event_type\"),\n",
    "    col(\"event.timestamp\").alias(\"timestamp\"),\n",
    ")\n",
    "\n",
    "final_df = final_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# For each event, calculate time since previous event (per user)\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "time_diff_df = final_df.withColumn(\n",
    "    \"previous_event_time\", lag(\"timestamp\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"time_diff_seconds\",\n",
    "    unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"previous_event_time\")),\n",
    ").withColumn(\n",
    "    \"time_diff_readable\",\n",
    "    from_unixtime(col(\"time_diff_seconds\"), \"HH:mm:ss\")\n",
    ")\n",
    "time_diff_df.filter(\"previous_event_time IS Not Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd0fe6a-042f-48d6-ac2c-db563bd3c2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complex Pivot with Aggregation and Ratio Calculation\n",
    "Dataset:\n",
    "transactions: user_id, category, amount, transaction_date\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "Pivot to get total amount spent per category as separate columns\n",
    "Add total amount and ratio of each category to total\n",
    "\"\"\"\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"u1\", \"grocery\", 120.0, \"2024-01-01\"),\n",
    "    (\"u1\", \"electronics\", 500.0, \"2024-01-02\"),\n",
    "    (\"u1\", \"grocery\", 80.0, \"2024-01-03\"),\n",
    "    (\"u2\", \"grocery\", 200.0, \"2024-01-01\"),\n",
    "    (\"u2\", \"fashion\", 300.0, \"2024-01-02\"),\n",
    "    (\"u3\", \"fashion\", 150.0, \"2024-01-03\"),\n",
    "    (\"u3\", \"electronics\", 700.0, \"2024-01-04\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)  # You can also make it DateType if needed\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "transactions_df = spark.createDataFrame(data, schema)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", to_date(\"transaction_date\"))\n",
    "\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ee59d4-fa17-480a-bbd8-201801e1701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivoted the dataframe to find each category amount\n",
    "pivoted_df = transactions_df.groupBy(\"user_id\").pivot(\"category\").sum(\"amount\")\n",
    "\n",
    "# Aggregating total amount per user\n",
    "aggregated_df = transactions_df.groupBy(\"user_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "# Joining both pivoted and aggregated dataframes\n",
    "joined_df = pivoted_df.join(aggregated_df, on=\"user_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "# Computing ratios for each category\n",
    "ratio_df = (joined_df\n",
    "            .withColumn(\"electronics_ratio\", round(coalesce(col(\"electronics\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"fashion_ratio\", round(coalesce(col(\"fashion\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"grocery_ratio\", round(coalesce(col(\"grocery\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            )\n",
    "\n",
    "ratio_df.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e9a426-5619-421a-bc17-0e0914075215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic way to calculate category ratios \n",
    "from pyspark.sql.functions import col, round, coalesce, lit\n",
    "\n",
    "# Existing columns\n",
    "basic_columns = joined_df.columns\n",
    "\n",
    "# List of category columns (excluding user_id and total_amount)\n",
    "category_columns = [c for c in basic_columns if c not in (\"user_id\", \"total_amount\")]\n",
    "\n",
    "# Start with existing columns\n",
    "final_cols = [col(\"user_id\")] + [col(c) for c in category_columns] + [col(\"total_amount\")]\n",
    "\n",
    "# Add ratio columns dynamically\n",
    "for cat in category_columns:\n",
    "    ratio_col_name = f\"{cat}_ratio\"\n",
    "    final_cols.append(\n",
    "        round(coalesce(col(cat), lit(0)) / col(\"total_amount\"), 2).alias(ratio_col_name)\n",
    "    )\n",
    "\n",
    "# Select all\n",
    "dynamic_ratio_df = joined_df.select(*final_cols)\n",
    "\n",
    "dynamic_ratio_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pysparkAssignments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
