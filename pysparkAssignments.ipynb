{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b353028b-779d-4e5f-8a7c-19a20cf5771c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a2dad4-a7e9-4d76-8f37-5467f99f0859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a small pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfcec84-18d0-4f07-a716-06c590c5aff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"P1\", \"North\", \"Electronics\", 1200.50, \"2025-04-01\"),\n",
    "    (\"P2\", \"North\", \"Electronics\", 950.75, \"2025-04-01\"),\n",
    "    (\"P3\", \"North\", \"Electronics\", 850.00, \"2025-04-01\"),\n",
    "    (\"P4\", \"North\", \"Electronics\", 200.00, \"2025-04-01\"),\n",
    "    (\"P1\", \"South\", \"Furniture\", 500.00, \"2025-04-01\"),\n",
    "    (\"P5\", \"South\", \"Furniture\", 1500.00, \"2025-04-01\"),\n",
    "    (\"P6\", \"South\", \"Furniture\", 700.00, \"2025-04-01\"),\n",
    "    (\"P7\", \"East\", \"Grocery\", 300.00, \"2025-04-01\"),\n",
    "    (\"P8\", \"East\", \"Grocery\", 1200.00, \"2025-04-01\"),\n",
    "    (\"P9\", \"East\", \"Grocery\", 1000.00, \"2025-04-01\"),\n",
    "    (\"P10\", \"East\", \"Grocery\", 50.00, \"2025-04-01\"),\n",
    "]\n",
    "\n",
    "# Define schema with sale_date as string\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sale_amount\", DoubleType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(data, schema)\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "sales_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1aba64-8a49-4f3c-ac9d-d682529e239c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each region and category, find:\n",
    "Top 3 selling products by revenue\n",
    "\"\"\"\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"total_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "top3_products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d28635f-dd5c-45a5-9d07-72f5887f5f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Average revenue of top 3 products\n",
    "\"\"\"\n",
    "# Join original sales data with top 3 products (based on total revenue)\n",
    "top3_sales_df = sales_df.join(top3_products_df.select(\"region\", \"category\", \"product_id\"), \n",
    "                              on=[\"region\", \"category\", \"product_id\"], how=\"inner\")\n",
    "\n",
    "# Calculate average revenue of those top 3 products per region-category\n",
    "avg_top3_df = top3_sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    avg(\"sale_amount\").alias(\"avg_top3_revenue\")\n",
    ")\n",
    "avg_top3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca7392a-e2ac-43ce-83d6-6fde5c1fe84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Percentage contribution of each top product to its category-region revenue\n",
    "# Aggregating total revenue for each product in category and region\n",
    "aggregated_df = sales_df.groupBy(\"region\", \"category\", \"product_id\").agg(\n",
    "    sum(\"sale_amount\").alias(\"top_product_revenue\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for products\n",
    "window_spec = Window.partitionBy(\"region\", \"category\").orderBy(desc(\"top_product_revenue\"))\n",
    "ranking_df = aggregated_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "top3_products_df = ranking_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Aggregating total reveenue across each region  and category\n",
    "total_reg_cate_df = sales_df.groupBy(\"region\", \"category\").agg(\n",
    "    sum(\"sale_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Joining total revenue and catgeory region level revenue\n",
    "joined_df = total_reg_cate_df.join(top3_products_df, on=[\"region\", \"category\"], how=\"inner\")\n",
    "\n",
    "# Contribution of top product in each category\n",
    "top_product_cont_df = joined_df.withColumn(\n",
    "    \"contribution\",\n",
    "    (col(\"top_product_revenue\") / col(\"total_revenue\")) * 100\n",
    ")\n",
    "top_product_cont_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e5b4d3-c72b-404e-946a-51d2b4d50b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect First and Last Purchase of Customer (Lifecycle Analysis)\n",
    "Dataset:\n",
    "orders: order_id, customer_id, order_date, order_amount\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "For each customer:\n",
    "Get first and last purchase date\n",
    "Calculate days between first and last order\n",
    "Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "Also, calculate average order value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e09fb00-042e-46bb-bae9-3941fd56727f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample schema for the 'orders' DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the 'orders' DataFrame\n",
    "data = [\n",
    "    (1, 101, \"2024-01-01\", 150.0),\n",
    "    (2, 101, \"2024-03-15\", 200.0),\n",
    "    (3, 102, \"2024-01-10\", 300.0),\n",
    "    (4, 103, \"2024-01-12\", 250.0),\n",
    "    (5, 104, \"2024-02-01\", 120.0),\n",
    "    (6, 104, \"2024-04-10\", 180.0)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert the 'order_date' column to date format\n",
    "orders_df = orders_df.withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97f38b-c165-4997-8f8a-ccca44e89936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get first and last purchase date\n",
    "first_last_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    min(\"order_date\").alias(\"first_purchase_date\"),\n",
    "    max(\"order_date\").alias(\"last_purchase_date\")\n",
    ")\n",
    "# Calculate days between first and last order\n",
    "day_diff_df = first_last_df.withColumn(\n",
    "    \"day_diff\",\n",
    "    datediff(col(\"last_purchase_date\"), col(\"first_purchase_date\"))\n",
    ")\n",
    "day_diff_df.show()\n",
    "\n",
    "# Flag if a customer is \"One-Time Buyer\" or \"Repeat Buyer\"\n",
    "customer_analysis_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    countDistinct(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "customers_flags_df = customer_analysis_df.withColumn(\n",
    "    \"buyer_type\",\n",
    "    when(col(\"total_orders\") > 1, \"Repeat_Buyer\").otherwise(\"One_Time_Buyer\")\n",
    ")\n",
    "customer_flags_df.show()\n",
    "\n",
    "# Also, calculate average order value\n",
    "avg_order_val_df = orders_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"order_amount\").alias(\"avg_order_value\")\n",
    ")\n",
    "window_spec = Window.orderBy(desc(\"avg_order_value\"))\n",
    "ranking_df = avg_order_val_df.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "ranking_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2089e4-a688-40ad-8f06-434c6641b4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Series Trend Analysis Per Product\n",
    "Dataset:\n",
    "daily_sales: product_id, sale_date, units_sold\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "For each product:\n",
    "Calculate 7-day moving average of sales\n",
    "Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "Flag spike days (where sales > 2 * moving avg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ba098d-cc01-4a0c-b686-5f8db0e16a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"units_sold\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (101, \"2024-04-01\", 50),\n",
    "    (101, \"2024-04-02\", 55),\n",
    "    (101, \"2024-04-03\", 53),\n",
    "    (101, \"2024-04-04\", 60),\n",
    "    (101, \"2024-04-05\", 65),\n",
    "    (101, \"2024-04-06\", 70),\n",
    "    (101, \"2024-04-07\", 75),\n",
    "    (101, \"2024-04-08\", 150),  # Spike\n",
    "    (101, \"2024-04-09\", 80),\n",
    "    (102, \"2024-04-01\", 20),\n",
    "    (102, \"2024-04-02\", 22),\n",
    "    (102, \"2024-04-03\", 25),\n",
    "    (102, \"2024-04-04\", 23),\n",
    "    (102, \"2024-04-05\", 24),\n",
    "    (102, \"2024-04-06\", 21),\n",
    "    (102, \"2024-04-07\", 19),\n",
    "    (102, \"2024-04-08\", 18),\n",
    "    (102, \"2024-04-09\", 35)  # Spike\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "daily_sales_df = spark.createDataFrame(data, schema)\n",
    "daily_sales_df = daily_sales_df.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "daily_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6938152e-c772-42e3-9511-dbeb82fbd548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate 7-day moving average of sales\n",
    "window = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(-6, 0)\n",
    "moving_avg_df = daily_sales_df.withColumn(\n",
    "    \"moving_avg\",\n",
    "    avg(\"units_sold\").over(window)\n",
    ")\n",
    "\n",
    "# Identify upward or downward trend using 3 consecutive increases/decreases\n",
    "window_spec = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "product_trends_df = daily_sales_df.withColumn(\n",
    "    \"prev_day_sale\",\n",
    "    lag(\"units_sold\").over(window_spec)\n",
    ")\n",
    "sales_analysis_df = product_trends_df.withColumn(\n",
    "    \"isHigher\",\n",
    "    when(col(\"units_sold\") > col(\"prev_day_sale\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Identifying consequtive streaks\n",
    "windows = Window.partitionBy(\"product\").orderBy(\"sale_date\")\n",
    "random_rows_df = sales_analysis_df.withColumn(\n",
    "    \"rn\", \n",
    "    row_number().over(windows)\n",
    ").filter(\"prev_day_sale is not null\")\n",
    "\n",
    "# Creating groups to find consequtives by substracting date and rownumber\n",
    "conse_grop_df = random_rows_df.withColumn(\n",
    "    \"grouping\",\n",
    "    date_sub(\"sale_date\", \"rn\")\n",
    ")\n",
    "aggregated_df = conse_grop_df.groupBy(\"product_id\", \"grouping\").agg(\n",
    "    sum(when(col(\"isHigher\") == 'Yes'),1).alias(\"total_growths\")\n",
    ")\n",
    "\n",
    "# Assigning ranks for consequtives for each product\n",
    "window_rank = Window.partitionBy(\"product\").orderBy(desc(\"total_growths\"))\n",
    "product_cons_rank_df = aggregated_df.withColumn(\"rank\", dense_rank().over(window_rank))\n",
    "product_cons_rank_df.show()\n",
    "\n",
    "# Flag spike days (where sales > 2 * moving avg)\n",
    "\n",
    "# Getting only last row per product to get moving average to join with original dataframe\n",
    "window_last = Window.partitionBy(\"product\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "product_moving_avg_df = moving_avg_df.withColumn(\"last_mov_avg\", last(\"moving_avg\").over(window_last))\n",
    "product_moving_avg_df = product_moving_avg_df.distinct()\n",
    "\n",
    "# Joining with original dataframe\n",
    "joined_df = daily_sales_df.join(product_moving_avg_df.select(\"product\", \"last_mov_avg\"), on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Filetering sales where greater than moving average\n",
    "filtered_df = joined_df.filter(col(\"units_sold\") > 2 * col(\"last_mov_avg\"))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba11c22-02f9-4d24-96d0-8c89a56db2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"user_id\": \"u1\",\n",
    "        \"events\": [\n",
    "            {\"event_type\": \"click\", \"timestamp\": \"2024-01-01T10:00:00\"},\n",
    "            {\"event_type\": \"purchase\", \"timestamp\": \"2024-01-01T10:05:00\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"events\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"event_type\", StringType(), True),\n",
    "                        StructField(\"timestamp\", StringType(), True),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "json_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Exploding the json dataframe with explode function\n",
    "exploded_df = json_df.withColumn(\"event\", explode(\"events\"))\n",
    "\n",
    "final_df = exploded_df.select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"event.event_type\").alias(\"event_type\"),\n",
    "    col(\"event.timestamp\").alias(\"timestamp\"),\n",
    ")\n",
    "\n",
    "final_df = final_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# For each event, calculate time since previous event (per user)\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "time_diff_df = final_df.withColumn(\n",
    "    \"previous_event_time\", lag(\"timestamp\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"time_diff_seconds\",\n",
    "    unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"previous_event_time\")),\n",
    ").withColumn(\n",
    "    \"time_diff_readable\",\n",
    "    from_unixtime(col(\"time_diff_seconds\"), \"HH:mm:ss\")\n",
    ")\n",
    "time_diff_df.filter(\"previous_event_time IS Not Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd0fe6a-042f-48d6-ac2c-db563bd3c2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complex Pivot with Aggregation and Ratio Calculation\n",
    "Dataset:\n",
    "transactions: user_id, category, amount, transaction_date\n",
    "\n",
    "üõ†Ô∏è Task:\n",
    "Pivot to get total amount spent per category as separate columns\n",
    "Add total amount and ratio of each category to total\n",
    "\"\"\"\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"u1\", \"grocery\", 120.0, \"2024-01-01\"),\n",
    "    (\"u1\", \"electronics\", 500.0, \"2024-01-02\"),\n",
    "    (\"u1\", \"grocery\", 80.0, \"2024-01-03\"),\n",
    "    (\"u2\", \"grocery\", 200.0, \"2024-01-01\"),\n",
    "    (\"u2\", \"fashion\", 300.0, \"2024-01-02\"),\n",
    "    (\"u3\", \"fashion\", 150.0, \"2024-01-03\"),\n",
    "    (\"u3\", \"electronics\", 700.0, \"2024-01-04\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)  # You can also make it DateType if needed\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "transactions_df = spark.createDataFrame(data, schema)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", to_date(\"transaction_date\"))\n",
    "\n",
    "transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ee59d4-fa17-480a-bbd8-201801e1701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivoted the dataframe to find each category amount\n",
    "pivoted_df = transactions_df.groupBy(\"user_id\").pivot(\"category\").sum(\"amount\")\n",
    "\n",
    "# Aggregating total amount per user\n",
    "aggregated_df = transactions_df.groupBy(\"user_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "# Joining both pivoted and aggregated dataframes\n",
    "joined_df = pivoted_df.join(aggregated_df, on=\"user_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "# Computing ratios for each category\n",
    "ratio_df = (joined_df\n",
    "            .withColumn(\"electronics_ratio\", round(coalesce(col(\"electronics\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"fashion_ratio\", round(coalesce(col(\"fashion\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            .withColumn(\"grocery_ratio\", round(coalesce(col(\"grocery\"), lit(0))/ col(\"total_amount\"), 2))\n",
    "            )\n",
    "\n",
    "ratio_df.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e9a426-5619-421a-bc17-0e0914075215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic way to calculate category ratios \n",
    "from pyspark.sql.functions import col, round, coalesce, lit\n",
    "\n",
    "# Existing columns\n",
    "basic_columns = joined_df.columns\n",
    "\n",
    "# List of category columns (excluding user_id and total_amount)\n",
    "category_columns = [c for c in basic_columns if c not in (\"user_id\", \"total_amount\")]\n",
    "\n",
    "# Start with existing columns\n",
    "final_cols = [col(\"user_id\")] + [col(c) for c in category_columns] + [col(\"total_amount\")]\n",
    "\n",
    "# Add ratio columns dynamically\n",
    "for cat in category_columns:\n",
    "    ratio_col_name = f\"{cat}_ratio\"\n",
    "    final_cols.append(\n",
    "        round(coalesce(col(cat), lit(0)) / col(\"total_amount\"), 2).alias(ratio_col_name)\n",
    "    )\n",
    "\n",
    "# Select all\n",
    "dynamic_ratio_df = joined_df.select(*final_cols)\n",
    "\n",
    "dynamic_ratio_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9b9793-e2cd-4714-ac39-79f25b9d7547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Financial Transactions ‚Äì Fraud Pattern Detection\n",
    "Dataset:\n",
    "transactions: transaction_id, account_id, amount, timestamp, location\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each account:\n",
    "Find if more than 3 transactions occur within 1 minute ‚Üí mark as \"suspicious burst\"\n",
    "Flag any transaction where location changes between two transactions < 5 minutes apart\n",
    "Output flagged transactions with reason (burst, suspicious_location)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da17a2a-b595-46b9-9527-86d0c8b332f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 101, 500, \"2025-04-29 10:00:00\", \"New York\"),\n",
    "    (2, 101, 300, \"2025-04-29 10:01:00\", \"New York\"),\n",
    "    (3, 101, 100, \"2025-04-29 10:02:00\", \"New York\"),\n",
    "    (4, 101, 200, \"2025-04-29 10:03:00\", \"New York\"),\n",
    "    (5, 101, 150, \"2025-04-29 10:05:00\", \"Los Angeles\"),\n",
    "    (6, 102, 700, \"2025-04-29 09:50:00\", \"Chicago\"),\n",
    "    (7, 102, 300, \"2025-04-29 09:51:00\", \"Chicago\"),\n",
    "    (8, 102, 200, \"2025-04-29 09:52:00\", \"Chicago\"),\n",
    "    (9, 102, 150, \"2025-04-29 09:53:00\", \"Chicago\"),\n",
    "    (10, 102, 100, \"2025-04-29 09:54:00\", \"Chicago\"),\n",
    "    (11, 103, 1000, \"2025-04-29 10:30:00\", \"San Francisco\"),\n",
    "    (12, 103, 200, \"2025-04-29 10:35:00\", \"San Francisco\"),\n",
    "    (13, 103, 300, \"2025-04-29 10:36:00\", \"San Francisco\"),\n",
    "    (14, 103, 100, \"2025-04-29 10:40:00\", \"Los Angeles\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a49dc75-3950-4c18-aa63-ae7912678a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find if more than 3 transactions occur within 1 minute ‚Üí mark as \"suspicious burst\"\n",
    "\n",
    "# Converting timestamp to seconds using unix_timestamp function\n",
    "df = df.withColumn(\"time_in_seconds\", unix_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# window specification for rolling 1 minute window\n",
    "window_spec = Window.partitionBy(\"account_id\").orderBy(\"timestamp\").rangeBetween(-60, 0)\n",
    "\n",
    "# Counting total transactions per account in rolling 1 window\n",
    "aggregated_df = df.withColumn(\n",
    "    \"total_transactions\", \n",
    "    count(\"transaction_id\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Filtering transactions less than or equal to 3 to flag them as suspecious\n",
    "suspecious_df = aggregated_df.filter(col(\"total_transactions\") > 3)\n",
    "suspecious_df = suspecious_df.withColumn(\"transaction_flag\", lit(\"suspecious_burst\"))\n",
    "\n",
    "suspecious_burst_df = suspecious_df.select(\n",
    "    col(\"transaction_id\"),\n",
    "    col(\"account_id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"location\"),\n",
    "    col(\"transaction_flag\")\n",
    ")\n",
    "\n",
    "# Flag any transaction where location changes between two transactions < 5 minutes apart\n",
    "\n",
    "# window specification to find previous transaction location and timestamp\n",
    "window = Window.partitionBy(\"account_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# Creating prev location and prev timestamp columns\n",
    "transaction_details_df = df.withColumn(\n",
    "    \"prev_location\",\n",
    "    lag(\"location\").over(window)\n",
    ").withColumn(\n",
    "    \"prev_timestamp\",\n",
    "    lag(\"timestamp\").over(window)\n",
    ")\n",
    "\n",
    "# Identifying differences between timestamp and prevtimestamp \n",
    "transaction_analysis_df = transaction_details_df.withColumn(\n",
    "    \"time_diff\",\n",
    "    (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"prev_timestamp\")))\n",
    ")\n",
    "\n",
    "# Filtering suspecious locations transactions\n",
    "suspecious_loc_df = transaction_analysis_df.filter(\n",
    "    (col(\"time_diff\") < 300) & \n",
    "    (col(\"location\") != col(\"prev_location\"))\n",
    ")\n",
    "\n",
    "suspecious_loc_df = suspecious_loc_df.withColumn(\"transaction_flag\", lit(\"suspecious_location\"))\n",
    "\n",
    "suspecious_loc_df = suspecious_loc_df.select(\n",
    "    col(\"transaction_id\"),\n",
    "    col(\"account_id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"location\"),\n",
    "    col(\"transaction_flag\")\n",
    ")\n",
    "\n",
    "suspecious_trans_df = suspecious_burst_df.union(suspecious_loc_df)\n",
    "suspecious_trans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3844a499-ece7-47b2-85bc-52e904c1d9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build Custom Aggregation Framework\n",
    "Dataset:\n",
    "metrics: system_id, metric_type, metric_value, timestamp\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "Build a function that can:\n",
    "Accept metric type(s) and a window (daily/weekly)\n",
    "Return aggregate summary (avg, min, max, count)\n",
    "Simulate custom rollups: CPU ‚Üí daily, Memory ‚Üí weekly, Disk ‚Üí monthly\n",
    "You‚Äôll use parameterized functions, groupBy with truncation, and pivot.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46192483-3660-42cc-8abe-ce60190c534b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CustomAggregation\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"system_id\", StringType(), True),\n",
    "    StructField(\"metric_type\", StringType(), True),\n",
    "    StructField(\"metric_value\", FloatType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"sys1\", \"CPU\", 75.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys1\", \"Memory\", 32.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys1\", \"Disk\", 500.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys1\", \"CPU\", 78.0, \"2025-05-01 10:00:00\"),\n",
    "    (\"sys1\", \"Memory\", 30.0, \"2025-05-01 10:00:00\"),\n",
    "    (\"sys2\", \"CPU\", 80.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys2\", \"Memory\", 28.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys2\", \"Disk\", 450.0, \"2025-05-01 09:00:00\"),\n",
    "    (\"sys2\", \"CPU\", 85.0, \"2025-05-02 09:00:00\"),\n",
    "    (\"sys2\", \"Memory\", 29.0, \"2025-05-02 09:00:00\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert timestamp string to actual timestamp format\n",
    "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13976fb7-3aa5-42ea-bc94-f79df4cbec65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomMetricAggregation:\n",
    "    def read_data(self, path):\n",
    "        try:\n",
    "            df = spark.read.format('csv').option('header', True).load(path)\n",
    "            df = df.withColumn(\"metric_value\", col(\"metric_value\").cast(\"double\")) \\\n",
    "                   .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurring while reading: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def metric_summary(self, df, metric_types: list, window_type: str):\n",
    "        result = {}\n",
    "\n",
    "        # Convert window_type string into Spark-supported values\n",
    "        if window_type == 'daily':\n",
    "            trunc_format = 'day'\n",
    "        elif window_type == 'weekly':\n",
    "            trunc_format = 'week'\n",
    "        elif window_type == 'monthly':\n",
    "            trunc_format = 'month'\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported window_type. Choose from 'daily', 'weekly', 'monthly'.\")\n",
    "\n",
    "        df_filtered = df.filter(col(\"metric_type\").isin(metric_types))\n",
    "        df_truncated = df_filtered.withColumn(\"window_start\", date_trunc(trunc_format, col(\"timestamp\")))\n",
    "\n",
    "        # Aggregate summary\n",
    "        aggregated_df = df_truncated.groupBy(\"system_id\", \"metric_type\", \"window_start\").agg(\n",
    "            avg(\"metric_value\").alias(\"avg_value\"),\n",
    "            min(\"metric_value\").alias(\"min_value\"),\n",
    "            max(\"metric_value\").alias(\"max_value\"),\n",
    "            count(\"metric_value\").alias(\"count\")\n",
    "        )\n",
    "\n",
    "        # Pivot the metric_type column\n",
    "        pivoted_df = df_truncated.groupBy(\"system_id\", \"window_start\").pivot(\"metric_type\").agg(\n",
    "            avg(\"metric_value\").alias(\"avg_value\")\n",
    "        )\n",
    "\n",
    "        result.update({\n",
    "            \"aggregated_df\": aggregated_df,\n",
    "            \"pivoted_df\": pivoted_df\n",
    "        })\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc94ea1f-a100-4274-8866-8f736af4ecc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Between Repeat Purchases by Category\n",
    "Dataset:\n",
    "purchases: user_id, product_category, purchase_time\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each user and category:\n",
    "Calculate days between purchases\n",
    "Flag customers who buy same category repeatedly within short spans (< 5 days)\n",
    "Output top 5 categories with the most repeat buyers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fd183a-5127-466b-a953-ad2f2f11b58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"u1\", \"Electronics\", \"2023-12-01 10:00:00\"),\n",
    "    (\"u1\", \"Electronics\", \"2023-12-03 09:30:00\"),\n",
    "    (\"u1\", \"Groceries\", \"2023-12-05 11:00:00\"),\n",
    "    (\"u1\", \"Electronics\", \"2023-12-10 12:15:00\"),\n",
    "    (\"u2\", \"Groceries\", \"2023-12-02 14:00:00\"),\n",
    "    (\"u2\", \"Groceries\", \"2023-12-04 14:00:00\"),\n",
    "    (\"u2\", \"Electronics\", \"2023-12-05 16:00:00\"),\n",
    "    (\"u3\", \"Books\", \"2023-12-01 09:00:00\"),\n",
    "    (\"u3\", \"Books\", \"2023-12-20 09:00:00\"),\n",
    "    (\"u3\", \"Books\", \"2023-12-22 10:00:00\"),\n",
    "    (\"u4\", \"Clothing\", \"2023-12-01 15:00:00\"),\n",
    "    (\"u4\", \"Clothing\", \"2023-12-02 16:00:00\"),\n",
    "    (\"u5\", \"Groceries\", \"2023-12-06 10:00:00\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"purchase_time\", StringType(), True),\n",
    "])\n",
    "\n",
    "purchases_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "purchases_df = purchases_df.withColumn(\"purchase_time\", to_timestamp(\"purchase_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "purchases_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44adaf1f-6ab3-4fed-a587-1a42b4485ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class categoryRepeatPurchases:\n",
    "    def read_data(self, path):\n",
    "        try:\n",
    "            df = spark.read.format('csv').option('header', True).load(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured while file reading {e}\")\n",
    "            return None\n",
    "        \n",
    "    def daysDiffInPurchases(self, df):\n",
    "        # Window specification to get previous purchase time\n",
    "        window_spec = Window.partitionBy(\"user_id\").orderBy(\"purchase_time\")\n",
    "        # Creating previous purchase time column by lag \n",
    "        prev_pur_df = df.withColumn(\n",
    "            \"prev_pur_time\", \n",
    "            lag(\"purchase_time\").over(window_spec)\n",
    "        )\n",
    "        # Identifying time differences by using unix timestamp\n",
    "        time_diff_df = prev_pur_df.withColumn(\n",
    "            \"time_diff\",\n",
    "            (unix_timestamp(col(\"purchase_time\")) - unix_timestamp(col(\"prev_pur_time\"))) / 3600\n",
    "        )    \n",
    "        return time_diff_df\n",
    "    \n",
    "    def flagingCustomers(self, df):\n",
    "        # Window spec to get previous category column and purchase time\n",
    "        window = Window.partitionBy(\"user_id\", \"product_category\").orderBy(\"purchase_time\")\n",
    "        # Creating columns prev category and prev time\n",
    "        updated_df = df.withColumn(\n",
    "            \"prev_pur_time\",\n",
    "            lag(\"purchase_time\").over(window)\n",
    "        )\n",
    "        # Identifying difference between current and previous purchases\n",
    "        diff_df = updated_df.withColumn(\n",
    "            \"day_diff\",\n",
    "            datediff(\"purchase_time\", \"prev_pur_time\")\n",
    "        )\n",
    "        # Filetering rows only having matching categories and day diff is 5 days\n",
    "        filtered_df = diff_df.filter(\n",
    "            col(\"day_diff\") <= 5\n",
    "        )\n",
    "        # Aggregating total records for each user \n",
    "        aggregated_df = filtered_df.groupBy(\"user_id\", \"product_category\").agg(\n",
    "            count(\"*\").alias(\"total_rows\")\n",
    "        )\n",
    "        # Flaging users where total rows > 1\n",
    "        flagging_df = aggregated_df.withColumn(\n",
    "            \"is_repeat_buyer\",\n",
    "            when(col(\"total_rows\") > 1, 'Yes').otherwise('No')\n",
    "        )\n",
    "        return flagging_df\n",
    "    \n",
    "    def top5Categories(self, df):\n",
    "        # Aggregating user level to get repeat buyers\n",
    "        user_agg_df = df.groupBy(\"user_id\", \"product_category\").agg(\n",
    "            count(\"*\").alias(\"total_rows\")\n",
    "        )\n",
    "        # Filtering users whose count > 1\n",
    "        filtered_df = user_agg_df.filter(col(\"total_rows\") > 1)\n",
    "        # Aggregating unique repeat users at category level\n",
    "        cat_agg_df = filtered_df.groupBy(\"product_category\").agg(\n",
    "            countDistinct(\"user_id\").alias(\"repeat_buyers\")\n",
    "        )\n",
    "        # Ranking categories based on total repeat buyers\n",
    "        window_rank = Window.orderBy(desc(\"repeat_buyers\"))\n",
    "        ranking_df = cat_agg_df.withColumn(\n",
    "            \"rank\",\n",
    "            dense_rank().over(window_rank)\n",
    "        )\n",
    "        # Filtering top 5 categories\n",
    "        top5_categories_df = ranking_df.filter(col(\"rank\") <= 5)\n",
    "        return top5_categories_df\n",
    "    \n",
    "# Instantiting class    \n",
    "user_beha_inst = categoryRepeatPurchases()\n",
    "\n",
    "# Reading file\n",
    "df = user_beha_inst.read_data(\"dbfs://FileStore/raw/user_purchases\")\n",
    "\n",
    "# Reading methods\n",
    "time_diff_df = user_beha_inst.daysDiffInPurchases(df)\n",
    "flagging_df = user_beha_inst.flagingCustomers(df)\n",
    "top5_categories_df = user_beha_inst.top5Categories(df)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99e9adc9-0d14-4cc2-a1cf-bfd70aa5a8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Course Completion & Dropout Pattern (Education Platform)\n",
    "Dataset:\n",
    "learning_events: user_id, course_id, module_number, event_type, event_time\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each course:\n",
    "Calculate average module completion rate\n",
    "Identify modules with highest drop-off\n",
    "Flag users who skipped modules (missing module_number in sequence)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97174ed-5952-484b-9a6e-7de76203eaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "# spark = SparkSession.builder.appName(\"CourseCompletion\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"course_id\", StringType(), True),\n",
    "    StructField(\"module_number\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", StringType(), True)  # Initially as string\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"U1\", \"C1\", 1, \"view\", \"2025-05-01 10:00:00\"),\n",
    "    (\"U1\", \"C1\", 2, \"complete\", \"2025-05-01 10:30:00\"),\n",
    "    (\"U1\", \"C1\", 4, \"view\", \"2025-05-01 11:00:00\"),  # skipped module 3\n",
    "    (\"U2\", \"C1\", 1, \"view\", \"2025-05-02 09:00:00\"),\n",
    "    (\"U2\", \"C1\", 2, \"drop\", \"2025-05-02 09:20:00\"),\n",
    "    (\"U3\", \"C1\", 1, \"complete\", \"2025-05-03 08:00:00\"),\n",
    "    (\"U3\", \"C1\", 2, \"complete\", \"2025-05-03 08:30:00\"),\n",
    "    (\"U3\", \"C1\", 3, \"complete\", \"2025-05-03 09:00:00\"),\n",
    "    (\"U3\", \"C1\", 4, \"complete\", \"2025-05-03 09:30:00\"),\n",
    "    (\"U1\", \"C2\", 1, \"view\", \"2025-05-04 10:00:00\"),\n",
    "    (\"U1\", \"C2\", 2, \"drop\", \"2025-05-04 10:15:00\"),\n",
    "    (\"U2\", \"C2\", 1, \"complete\", \"2025-05-05 11:00:00\"),\n",
    "    (\"U2\", \"C2\", 2, \"complete\", \"2025-05-05 11:30:00\"),\n",
    "    (\"U2\", \"C2\", 3, \"view\", \"2025-05-05 12:00:00\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "learning_events_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "learning_events_df = learning_events_df.withColumn(\"event_time\", to_timestamp(\"event_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "\n",
    "# Show raw DataFrame with string timestamps\n",
    "learning_events_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f03a22-09d1-4b01-9bef-3e32cdfc4d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flag users who skipped modules (missing module_number in sequence)\n",
    "\n",
    "# Window specification to get previous module number\n",
    "window_spec = Window.partitionBy(\"user_id\", \"course_id\").orderBy(\"module_number\")\n",
    "prev_modules_df = learning_events_df.withColumn(\n",
    "    \"prev_module\",\n",
    "    lag(\"module_number\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Identifying gaps in module numbers\n",
    "module_gaps_df = prev_modules_df.withColumn(\n",
    "    \"module_gap\",\n",
    "    (col(\"module_number\") - col(\"prev_module\"))\n",
    ")\n",
    "\n",
    "# Flaging users who has > 1 gap between modules\n",
    "skipped_users_df = module_gaps_df.withColumn(\n",
    "    \"is_skipped\",\n",
    "    when(col(\"module_gap\") > 1, 'Yes').otherwise('No')\n",
    ")\n",
    "\n",
    "# Filter the users whose have is_skipped is yes\n",
    "filtered_df = skipped_users_df.filter(col(\"is_skipped\") == 'Yes').distinct()\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3faf732-80ae-47ce-a8f5-a8672e442ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify modules with highest drop-off\n",
    "\n",
    "# Aggregating total distinct users who dropped the module\n",
    "agg_df = learning_events_df.groupBy(\"course_id\", \"module_number\").agg(\n",
    "    countDistinct(when(col(\"event_type\") == \"drop\", col(\"user_id\"))).alias(\"users_dropped\")\n",
    ")\n",
    "\n",
    "# Ranking modules on each course based on users dropped\n",
    "window_rank = Window.partitionBy(\"course_id\").orderBy(desc(\"users_dropped\"))\n",
    "ranking_modules_df = agg_df.withColumn(\n",
    "    \"rank\",\n",
    "    dense_rank().over(window_rank)\n",
    ")\n",
    "\n",
    "# Filtered modules which has highest rank\n",
    "filtered_df = ranking_modules_df.filter(col(\"rank\") == 1)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "155c70fe-6de1-4652-859a-37a4bb84f843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average module completion rate\n",
    "\n",
    "# Aggregating completion and total rows \n",
    "agg_df = learning_events_df.groupBy(\"course_id\", \"module_number\").agg(\n",
    "    countDistinct(when(col(\"event_type\") == \"complete\", col(\"user_id\"))).alias(\"users_completed\"),\n",
    "    countDistinct(\"user_id\").alias(\"users_total\")\n",
    ")\n",
    "\n",
    "# Calculating completion rate\n",
    "comp_rate_df = agg_df.withColumn(\n",
    "    \"completion_rate\",\n",
    "    (col(\"total_com_rows\") / col(\"total_rows\"))\n",
    ")\n",
    "\n",
    "# Aggregating average completion rate for each course\n",
    "avg_com_rate_df = comp_rate_df.groupBy(\"course_id\").agg(\n",
    "    avg(\"completion_rate\").alias(\"avg_comp_rate\")\n",
    ")\n",
    "avg_com_rate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647d5e10-a035-41c9-8876-e02d68a8a5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect Salary Leaks in Departments\n",
    "Dataset:\n",
    "employees: emp_id, name, department, salary, joining_date\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each department:\n",
    "Calculate average salary\n",
    "Identify employees with salary 3x more than average (potential anomaly)\n",
    "Flag departments with such anomalies and report total impact\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc9616b-d1fb-4d41-98d0-6e98e1685ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema with joining_date as string\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"joining_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data with dates as string\n",
    "data = [\n",
    "    (\"E001\", \"Alice\",   \"HR\",        50000, \"2020-01-15\"),\n",
    "    (\"E002\", \"Bob\",     \"HR\",        52000, \"2021-03-12\"),\n",
    "    (\"E003\", \"Carol\",   \"HR\",       180000, \"2019-07-23\"),  # anomaly\n",
    "    (\"E004\", \"David\",   \"IT\",        70000, \"2018-10-01\"),\n",
    "    (\"E005\", \"Eve\",     \"IT\",        73000, \"2020-06-20\"),\n",
    "    (\"E006\", \"Frank\",   \"IT\",       210000, \"2021-08-30\"),  # anomaly\n",
    "    (\"E007\", \"Grace\",   \"Finance\",   60000, \"2019-01-10\"),\n",
    "    (\"E008\", \"Hank\",    \"Finance\",   62000, \"2020-11-11\"),\n",
    "    (\"E009\", \"Ivy\",     \"Finance\",   61000, \"2022-01-05\"),\n",
    "    (\"E010\", \"John\",    \"Finance\",   65000, \"2023-04-19\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employees_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert joining_date string to timestamp\n",
    "employees_df = employees_df.withColumn(\"joining_date\", to_timestamp(\"joining_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "employees_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772477c8-431b-46dd-91c0-22ffb93b33f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average salary\n",
    "avg_dep_sal_df = employees_df.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_dept_sal\")\n",
    ")\n",
    "avg_dep_sal_df.show()\n",
    "\n",
    "# Identify employees with salary 3x more than average (potential anomaly)\n",
    "\n",
    "# Joining both original dataframe with avg dep sal dataframe\n",
    "joined_df = employees_df.join(avg_dep_sal_df, on=\"department\", how=\"inner\")\n",
    "\n",
    "# Filtering employees whose salary is greater than 3 times of there dep avg salary\n",
    "filtered_df = joined_df.filter(col(\"salary\") > 3 * col(\"avg_dept_sal\"))\n",
    "filtered_df.show()\n",
    "\n",
    "# Flagging departments which has anomalies\n",
    "flaging_df = joined_df.withColumn(\n",
    "    \"is_anomalied\",\n",
    "    when(col(\"salary\") > 3 * col(\"avg_dept_sal\"), 'yes').otherwise('No')\n",
    ")\n",
    "anomalied_dept_df = flaging_df.select(\"department\", \"is_anomalied\").filter(col(\"is_anomalied\") == 'Yes')\n",
    "\n",
    "# Filtered only anomalies records from dataframe\n",
    "flaging_df = flaging_df.filter(col(\"is_anomalied\") == 'Yes')\n",
    "\n",
    "# Total impact over anomalies departments\n",
    "dept_analysis_df = flaging_df.groupBy(\"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_anomalies_amount\"),\n",
    "    count(\"employee_id\").alias(\"total_employees\")\n",
    ")\n",
    "\n",
    "# Joining with dept avg dataframe to get avg salary to calculate impact\n",
    "dept_analysis_with_avg_df = dept_analysis_df.join(avg_dep_sal_df, on=\"department\", how=\"inner\")\n",
    "dept_analysis_with_avg_df = dept_analysis_with_avg_df.withColumn(\n",
    "    \"ExpectedSalary\",\n",
    "    (col(\"avg_dept_sal\") * col(\"total_employees\"))\n",
    ")\n",
    "total_impact_df = dept_analysis_with_avg_df.withColumn(\n",
    "    \"TotalImpact\",\n",
    "    (col(\"total_anomalies_amount\") - col(\"ExpectedSalary\"))\n",
    ")\n",
    "total_impact_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed3f15bd-8606-4034-92ae-86ec3fffd0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Employee Promotion Path Tracking\n",
    "Dataset:\n",
    "employee_roles: emp_id, role, start_date, end_date\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each employee:\n",
    "Track complete promotion path in order.\n",
    "Calculate how long they stayed in each role.\n",
    "Find employees with more than 2 promotions in 3 years.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b459c858-6781-43b8-a529-d99764968d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"E001\", \"Junior Developer\", \"2018-01-01\", \"2019-06-30\"),\n",
    "    (\"E001\", \"Developer\",        \"2019-07-01\", \"2020-12-31\"),\n",
    "    (\"E001\", \"Senior Developer\", \"2021-01-01\", \"2022-06-30\"),\n",
    "    (\"E001\", \"Tech Lead\",        \"2022-07-01\", \"2024-01-01\"),\n",
    "    \n",
    "    (\"E002\", \"Analyst\",          \"2020-01-01\", \"2022-12-31\"),\n",
    "    (\"E002\", \"Senior Analyst\",   \"2023-01-01\", \"2024-12-31\"),\n",
    "\n",
    "    (\"E003\", \"Intern\",           \"2021-06-01\", \"2022-05-31\"),\n",
    "    (\"E003\", \"Junior Dev\",       \"2022-06-01\", \"2023-05-31\"),\n",
    "    (\"E003\", \"Developer\",        \"2023-06-01\", \"2024-06-01\"),\n",
    "    (\"E003\", \"Senior Dev\",       \"2024-06-02\", \"2025-05-31\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", StringType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"start_date\", StringType(), True),\n",
    "    StructField(\"end_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "employee_roles_df = spark.createDataFrame(data, schema=schema)\n",
    "employee_roles_df = employee_roles_df.withColumn(\"start_date\", to_date(\"start_date\")) \\\n",
    "    .withColumn(\"end_date\", to_date(\"end_date\"))\n",
    "\n",
    "employee_roles_df.show(truncate=False)\n",
    "employee_roles_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5abf88a-e54e-439f-9a9b-e7031b2c177f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Track complete promotion path in order.\n",
    "\n",
    "# Window specification for complete path of roles for employees\n",
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"start_date\")\n",
    "emp_roles_order_df = employee_roles_df.withColumn(\n",
    "    \"rn\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "emp_roles_order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f3d551-3f12-4242-8b8f-9cef03246bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate how long they stayed in each role.\n",
    "role_durations_df = employee_roles_df.withColumn(\n",
    "    \"role_duration\",\n",
    "    datediff(\"end_date\", \"start_date\")\n",
    ")\n",
    "role_durations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aec5655-fdf1-49f3-a6d8-8849f5d6bc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find employees with more than 2 promotions in 3 years.\n",
    "# Window spec for aggregating minimum start date \n",
    "window = Window.partitionBy(\"emp_id\")\n",
    "min_start_date_df = employee_roles_df.withColumn(\n",
    "    \"min_start_date\",\n",
    "    min(\"start_date\").over(window)\n",
    ")\n",
    "# Filtering 3 years promotions for employees\n",
    "filtered_df = min_start_date_df.filter(\n",
    "    (col(\"start_date\") >= col(\"min_start_date\")) &\n",
    "    (col(\"start_date\") <= add_months(col(\"min_start_date\"), 36))\n",
    ")\n",
    "\n",
    "# Aggregating total roles for employees\n",
    "emp_total_promotions_df = filtered_df.groupBy(\"emp_id\").agg(\n",
    "    countDistinct(\"role\").alias(\"total_promotions\")\n",
    ")\n",
    "\n",
    "# Filtering employees who has more than 2 promotions\n",
    "final_df = emp_total_promotions_df.filter(col(\"total_promotions\") > 2)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ea7f88-f210-4f23-be1b-068096242fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Customer Repeat Purchase Frequency\n",
    "Dataset:\n",
    "orders: order_id, customer_id, order_date\n",
    "\n",
    "üõ†Ô∏è Tasks:\n",
    "For each customer:\n",
    "Calculate number of days between purchases.\n",
    "\n",
    "Label customers:\n",
    "frequent: buys every <10 days\n",
    "infrequent: every 10‚Äì30 days\n",
    "dormant: >30 days\n",
    "Find average frequency per segment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addd4c0d-1076-49c0-8577-1bb7dd18e54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema with order_date as StringType\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True)  # date as string\n",
    "])\n",
    "\n",
    "# Sample data with string dates\n",
    "data = [\n",
    "    (1, 101, \"2024-01-01\"),\n",
    "    (2, 101, \"2024-01-08\"),\n",
    "    (3, 101, \"2024-01-17\"),\n",
    "    (4, 102, \"2024-02-01\"),\n",
    "    (5, 102, \"2024-03-05\"),\n",
    "    (6, 102, \"2024-06-10\"),\n",
    "    (7, 103, \"2024-01-01\"),\n",
    "    (8, 103, \"2024-01-20\"),\n",
    "    (9, 103, \"2024-02-25\"),\n",
    "    (10, 104, \"2024-05-01\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert string to date\n",
    "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e508573-800a-4b25-8a08-88b44ae71e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate number of days between purchases.\n",
    "\n",
    "# Window specification to get previous order date\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "prev_purc_df = orders_df.withColumn(\n",
    "    \"prev_order_date\",\n",
    "    lag(\"order_date\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Day differences between dates\n",
    "day_diff_df = prev_purc_df.withColumn(\n",
    "    \"day_diff\",\n",
    "    datediff(col(\"order_date\"), col(\"prev_order_date\"))\n",
    ")\n",
    "\n",
    "# Labelling purchases based on differencs\n",
    "labelled_df = day_diff_df.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"day_diff\") < 10, 'frequent')\n",
    "    .when((col(\"day_diff\") >= 10) & (col(\"day_diff\") <= 30), 'infrequent')\n",
    "    .otherwise('dormant')\n",
    ").filter(\"prev_order_date is not null\")\n",
    "labelled_df.show()\n",
    "\n",
    "# Aggregating average frequency over segments\n",
    "aggregated_df = labelled_df.groupBy(\"label\").agg(\n",
    "    avg(\"day_diff\").alias(\"avg_frequency\")\n",
    ")\n",
    "aggregated_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pysparkAssignments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
